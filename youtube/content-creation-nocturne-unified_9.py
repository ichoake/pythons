"""
Content Creation Nocturne Unified 9

This module provides functionality for content creation nocturne unified 9.

Author: Auto-generated
Date: 2025-11-01
"""


import logging

logger = logging.getLogger(__name__)


# Constants
CONSTANT_100 = 100
CONSTANT_300 = 300
CONSTANT_375 = 375
CONSTANT_649 = 649
CONSTANT_1000 = 1000
CONSTANT_1024 = 1024
CONSTANT_2025 = 2025

#!/usr/bin/env python3
"""
UNIFIED SOLUTION - Comprehensive File Manager

This file combines the best functionality from CONSTANT_375 files:
- ULTIMATE_FILE_MANAGER_USAGE.md
- consolidate_duplicates.py
- deep_analysis_merge.py
- DUPLICATE_CONSOLIDATION_SUMMARY.md
- intelligent_analyzer_merger.py
- deep_read_analysis_report.md
- COMBINATION_SUMMARY.md
- mp3-process.md
- ULTIMATE_FILE_MANAGER.py
- CONSOLIDATION_SUMMARY.md
- CONSOLIDATION_REPORT.md
- process_music.sh
- mp4-txt.sh
- analyze-prompt1.py.json
- file-sort.py
- song-process.py
- analyze_1.py
- analyze-mp3-transcript-prompts.py
- CONSOLIDATION_REPORT.md
- mp4-transcript.py
- analyze-mp3-transcript-prompts_.py
- mp4-transcript_1.py
- README_10.md
- song--analyze-keys.py
- mp3_processor.py
- advanced_transcriber_merged.py
- DETAILED_ANALYSIS.json
- untitled_merged.py
- trans.sh
- consolidate_scripts_merged.py
- transcribe_trans-gpot35.py
- start-with-openai_merged.py
- mp3-process.py.json
- html-auto-img-gallery_merged.py
- analyze-prompt.py
- suno__merged.py
- advanced_content_generator_merged.py
- generate_album_pages.py
- organize_music_library_merged.py
- splt-1_merged.py
- split-2_merged.py
- story-section-gpt_merged.py
- analyze-prompts.py
- file-sort_merged.py
- analyzer.py
- story-key-trans.py
- groq_merged.py
- robust_sort_and_dedupe.py
- SORTING_SUMMARY.md
- suno-csv-card-html-seo1.py
- analyze-prompt_1.py
- cover_merged.py
- analyze_merged.py
- trans_merged.py
- QuizPrompts_merged.py
- csv_merged.py
- analyze-mp3-transcript-prompts_1.py
- analyze-shorts-info.py
- image-gpt_merged.py
- README.md
- create_video.py
- mp4-mp3-analyze2.py
- advanced_content_analyzer.py
- analyze-shorts-info_1.py
- mp3-process.md.json
- story-section-gpt.py
- suno-csv-card-html-seo2.py
- suno-csv-card-html-seo.py
- yt-meta_merged.py
- analyze_1_1.py
- generate_songs_csv_merged.py
- deep_read_analysis_report.md
- comprehensive_merge.py
- analyzer_1.py
- transcribe.py
- sora-song.py
- consolidate_scripts.py
- analyze11.py
- robust_sort_and_dedupe_merged.py
- auto-image-gallery_merged.py
- sort_python_files_merged.py
- mp3-trans-storytime.py
- comprehensive_merge_merged.py
- organize_music_library.py
- generate_album_html-pages_fixed.py
- transcribe_.py
- advanced_content_analyzer_merged.py
- advanced_sort_and_dedupe.py
- generate_songs_csv.py
- advanced_web_scraper_merged.py
- imgupscale_merged.py
- analyze-shorts_1.py
- generate_merged.py
- mp3-process.md
- yplaylist_merged.py
- analyze_.py
- mp4-mp3-analyze.py
- sort_python_files.py
- ytcsv_merged.py
- analyze-shorts.py
- test_environment_merged.py
- fancyimg_merged.py
- song-process2.py
- sora_merged.py
- README_23.md
- CONSOLIDATION_SUMMARY.md
- analyze-prompts_1.py
- img-origin-date_merged.py
- analyze_img_csv.py
- untitled.py
- analyzer-prompt.py
- test_environment.py
- analyze.py
- analyze_6.py
- cover2_merged.py
- SORTING_REPORT.md
- analyze-prompt1.py
- CONSOLIDATION_REPORT.md
- analyze-prompt1.py.json
- file-sort.py
- story-section-gpt_1.py
- readme_README_10.md
- song-process.py
- analyze_1.py
- transcribe_variants_mp3-trans-storytime.py
- analyze-mp3-transcript-prompts.py
- sort_python_files_sort_python_files.py
- advanced_sort_and_dedupe_1.py
- CONSOLIDATION_REPORT.md
- analyze-prompts_analyze-prompts_1.py
- mp4-transcript.py
- analyze-mp3-transcript-prompts_.py
- robust_sort_and_dedupe_robust_sort_and_dedupe.py
- mp4-transcript_1.py
- sora-song_sora-song_merged.py
- transcribe_variants_mp4-transcript.py
- song--analyze-keys.py
- mp3_processor.py
- DETAILED_ANALYSIS.json
- content_analyzer_advanced_content_analyzer.py
- transcribe_variants_transcribe.py
- analyze_variants_analyze_variants_analyze_1.py
- analyze-shorts_analyze-shorts_1.py
- story-section-gpt_story-section-gpt.py
- trans.sh
- suno_variants_suno-csv-card-html-seo1.py
- transcribe_trans-gpot35.py
- analyze_variants_analyze_variants_song--analyze-keys.py
- transcribe_variants_story-key-trans.py
- mp3-process.py.json
- organize_music_library_organize_music_library.py
- analyze_analyze_6.py
- generate_songs_csv_generate_songs_csv.py
- analyze_variants_analyze_variants_analyze-prompt_1.py
- generate_variants_create_video.py
- analyze-prompt.py
- quiz22s_Quiz22s_merged.py
- quiz-20_quiz-20_merged.py
- test_1.py
- process_variants_song-process2.py
- analyze_variants_analyze_2.py
- consolidate_scripts_consolidate_scripts.py
- generate_album_pages.py
- analyze_variants_analyze_variants_analyze-prompt1.py
- file-sort_1.py
- analyze_variants_analyze_variants_analyze-shorts-info_1.py
- analyze_variants_analyze_variants_analyze-shorts-info.py
- process_music.sh
- analyze_variants_analyze_variants_analyze-prompt.py
- analyze-prompt_analyze-prompt.py
- analyze_variants_analyze_variants_analyze 6.py
- analyze-prompts.py
- mp4-txt.sh
- analyze_variants_analyze_variants_analyze_img_csv.py
- analyze_variants_analyze_variants_analyzer-prompt.py
- file-sort_file-sort.py
- analyze-shorts-info_analyze-shorts-info.py
- comprehensive_merge_1.py
- analyze_analyze_.py
- analyzer.py
- generate_songs_csv_1_1.py
- story-key-trans.py
- generate_variants_generate_songs_csv 1.py
- robust_sort_and_dedupe.py
- analyze_variants_analyze_variants_analyze (1).py
- generate_variants_generate_album_html-pages_fixed.py
- SORTING_SUMMARY.md
- analyze_variants_analyze_variants_mp4-mp3-analyze.py
- suno-csv-card-html-seo1.py
- analyze-prompt_1.py
- analyze_variants_analyze_variants_analyze-mp3-transcript-prompts_1.py
- test_environment_test_environment.py
- analyze-mp3-transcript-prompts_analyze-mp3-transcript-prompts_.py
- analyze-mp3-transcript-prompts_1.py
- trans_trans.sh
- comprehensive_merge_comprehensive_merge.py
- analyze-shorts-info.py
- trans_1.sh
- test_environment_1.py
- create_video.py
- mp4-mp3-analyze2.py
- advanced_content_analyzer.py
- analyze-shorts-info_1.py
- mp3-process.md.json
- story-section-gpt.py
- suno-csv-card-html-seo2.py
- analyze_variants_analyze_variants_analyze-shorts_1.py
- suno-csv-card-html-seo.py
- analyze_1_1.py
- process_music_1.sh
- README_28.md
- analyzer_analyzer_1.py
- README_2.md
- organize_music_library_1.py
- analyze-prompts_1_1.py
- analyze_variants_analyze_variants_analyze11.py
- deep_read_analysis_report.md
- comprehensive_merge.py
- analyzer_1.py
- consolidate_scripts_1.py
- analyze_variants_analyze_variants_analyze-shorts.py
- transcribe_transcribe_.py
- transcribe.py
- advanced_content_analyzer_1.py
- suno_suno_merged.py
- sora-song.py
- consolidate_scripts.py
- analyze11.py
- analyze_variants_analyze_variants_analyzer 1.py
- untitled_1.py
- generate_generate__merged.py
- analyze_variants_analyze-1.py
- analyze_variants_analyze_variants_analyze 1.py
- analyze_variants_analyze_variants_analyze-mp3-transcript-prompts (1).py
- mp3-trans-storytime.py
- process_variants_mp3_processor.py
- organize_music_library.py
- generate_album_html-pages_fixed.py
- mp4-transcript_mp4-transcript_1.py
- transcribe_.py
- suno_variants_suno-csv-card-html-seo2.py
- transcribe_variants_transcribe (1).py
- robust_sort_and_dedupe_1.py
- mp4-txt_1.sh
- advanced_sort_and_dedupe.py
- analyze_analyze.py
- analyze_variants_analyze_variants_analyze-prompt (1).py
- analyze-promptr_1.py
- analyze_variants_analyze_variants_analyze-mp3-transcript-prompts.py
- analyze-shorts_1.py
- mp3-process.md
- transcribe_variants_trans-gpot35.py
- analyze_.py
- mp4-mp3-analyze.py
- sort_python_files.py
- generate_variants_generate_album_pages.py
- analyze-shorts.py
- analyze_analyze_1.py
- analyze_variants_analyze_variants_mp4-mp3-analyze2.py
- transcribe_variants_mp4-transcript_1.py
- analyze_variants_analyze_variants_analyzer.py
- suno_variants_suno-csv-card-html-seo.py
- song-process2.py
- sora-song_1.py
- CONSOLIDATION_SUMMARY.md
- analyze_analyze__merged.py
- mp3-4-transcribe_1.py
- analyze-prompts_1.py
- analyze_img_csv.py
- untitled.py
- untitled_untitled.py
- analyzer-prompt.py
- quiz22sec_Quiz22sec_merged.py
- test_environment.py
- analyze.py
- analyze_variants_analyze_variants_analyze-prompts.py
- analyze-mp3-transcript-prompts_analyze-mp3-transcript-prompts.py
- analyze_6.py
- story-key-trans copy_1.py
- README_13.md
- readme_README_23.md
- process_variants_song-process.py
- SORTING_REPORT.md
- analyze-prompt1.py
- sort_python_files_1.py
- mp4-txt.sh
- process_music.sh
- song-process.py
- merged_11_files.py
- DEEP_ANALYSIS_REPORT.md
- mp4-transcript.py
- song--analyze-keys.py
- mp3_processor.py
- advanced_transcriber_merged.py
- untitled_merged.py
- consolidate_scripts_merged.py
- transcribe_trans-gpot35.py
- start-with-openai_merged.py
- html-auto-img-gallery_merged.py
- suno__merged.py
- merged_7_files.py
- merged_6_files.py
- advanced_content_generator_merged.py
- generate_album_pages.py
- organize_music_library_merged.py
- splt-1_merged.py
- split-2_merged.py
- story-section-gpt_merged.py
- analyze-prompts.py
- file-sort_merged.py
- analyzer.py
- story-key-trans.py
- groq_merged.py
- suno-csv-card-html-seo1.py
- analyze-prompt_1.py
- cover_merged.py
- analyze_merged.py
- trans_merged.py
- QuizPrompts_merged.py
- csv_merged.py
- analyze-mp3-transcript-prompts_1.py
- image-gpt_merged.py
- create_video.py
- mp4-mp3-analyze2.py
- analyze-shorts-info_1.py
- merged_4_files.py
- merged_5_files.py
- suno-csv-card-html-seo2.py
- suno-csv-card-html-seo.py
- yt-meta_merged.py
- analyze_1_1.py
- generate_songs_csv_merged.py
- transcribe.py
- merged_18_files.py
- sora-song.py
- analyze11.py
- y_merged.py
- merged_13_files.py
- robust_sort_and_dedupe_merged.py
- auto-image-gallery_merged.py
- sort_python_files_merged.py
- mp3-trans-storytime.py
- comprehensive_merge_merged.py
- generate_album_html-pages_fixed.py
- advanced_content_analyzer_merged.py
- s_merged.py
- advanced_sort_and_dedupe.py
- gen_merged.py
- advanced_web_scraper_merged.py
- imgupscale_merged.py
- generate_merged.py
- yplaylist_merged.py
- merged_14_files.py
- mp4-mp3-analyze.py
- ytcsv_merged.py
- analyze-shorts.py
- test_environment_merged.py
- fancyimg_merged.py
- song-process2.py
- sora_merged.py
- merged_3_files.py
- merged_2_files.py
- img-origin-date_merged.py
- analyze_img_csv.py
- analyzer-prompt.py
- cover2_merged.py
- analyze-prompt1.py
- MERGE_ANALYSIS.json
- MERGE_REPORT.md
- SORTING_SUMMARY.md
- SORTING_REPORT.md
- analyze-prompt1.py.json
- DETAILED_ANALYSIS.json
- mp3-process.py.json
- mp3-process.md.json

Generated: CONSTANT_2025-10-15 12:26:36
Total files analyzed: CONSTANT_649
Total functionality groups: 7

This unified solution provides:
- File processing and conversion
- Content analysis and generation
- Transcription and audio processing
- Web scraping and data extraction
- File organization and management
- Comprehensive error handling and logging
"""

# Copyright (c) 2014 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""
This script is intended for use as a GYP_GENERATOR. It takes as input (by way of
the generator flag config_path) the path of a json file that dictates the files
and targets to search for. The following keys are supported:
files: list of paths (relative) of the files to search for.
test_targets: unqualified target names to search for. Any target in this list
that depends upon a file in |files| is output regardless of the type of target
or chain of dependencies.
additional_compile_targets: Unqualified targets to search for in addition to
test_targets. Targets in the combined list that depend upon a file in |files|
are not necessarily output. For example, if the target is of type none then the
target is not output (but one of the descendants of the target will be).

The following is output:
error: only supplied if there is an error.
compile_targets: minimal set of targets that directly or indirectly (for
  targets of type none) depend on the files in |files| and is one of the
  supplied targets or a target that one of the supplied targets depends on.
  The expectation is this set of targets is passed into a build step. This list
  always contains the output of test_targets as well.
test_targets: set of targets from the supplied |test_targets| that either
  directly or indirectly depend upon a file in |files|. This list if useful
  if additional processing needs to be done for certain targets after the
  build, such as running tests.
status: outputs one of three values: none of the supplied files were found,
  one of the include files changed so that it should be assumed everything
  changed (in this case test_targets and compile_targets are not output) or at
  least one file was found.
invalid_targets: list of supplied targets that were not found.

Example:
Consider a graph like the following:
  A     D
 / \
B   C
A depends upon both B and C, A is of type none and B and C are executables.
D is an executable, has no dependencies and nothing depends on it.
If |additional_compile_targets| = ["A"], |test_targets| = ["B", "C"] and
files = ["b.cc", "d.cc"] (B depends upon b.cc and D depends upon d.cc), then
the following is output:
|compile_targets| = ["B"] B must built as it depends upon the changed file b.cc
and the supplied target A depends upon it. A is not output as a build_target
as it is of type none with no rules and actions.
|test_targets| = ["B"] B directly depends upon the change file b.cc.

Even though the file d.cc, which D depends upon, has changed D is not output
as it was not supplied by way of |additional_compile_targets| or |test_targets|.

If the generator flag analyzer_output_path is specified, output is written
there. Otherwise output is written to stdout.

In Gyp the "all" target is shorthand for the root targets in the files passed
to gyp. For example, if file "a.gyp" contains targets "a1" and
"a2", and file "b.gyp" contains targets "b1" and "b2" and "a2" has a dependency
on "b2" and gyp is supplied "a.gyp" then "all" consists of "a1" and "a2".
Notice that "b1" and "b2" are not in the "all" target as "b.gyp" was not
directly supplied to gyp. OTOH if both "a.gyp" and "b.gyp" are supplied to gyp
then the "all" target includes "b1" and "b2".
"""


from pathlib import Path
import json
import os
import posixpath

import gyp.common

debug = False

found_dependency_string = "Found dependency"
no_dependency_string = "No dependencies"
# Status when it should be assumed that everything has changed.
all_changed_string = "Found dependency (all)"

# MatchStatus is used indicate if and how a target depends upon the supplied
# sources.
# The target's sources contain one of the supplied paths.
MATCH_STATUS_MATCHES = 1
# The target has a dependency on another target that contains one of the
# supplied paths.
MATCH_STATUS_MATCHES_BY_DEPENDENCY = 2
# The target's sources weren't in the supplied paths and none of the target's
# dependencies depend upon a target that matched.
MATCH_STATUS_DOESNT_MATCH = 3
# The target doesn't contain the source, but the dependent targets have not yet
# been visited to determine a more specific status yet.
MATCH_STATUS_TBD = 4

generator_supports_multiple_toolsets = gyp.common.CrossCompileRequested()

generator_wants_static_library_dependencies_adjusted = False

generator_default_variables = {}
for dirname in [
    "INTERMEDIATE_DIR",
    "SHARED_INTERMEDIATE_DIR",
    "PRODUCT_DIR",
    "LIB_DIR",
    "SHARED_LIB_DIR",
]:
    generator_default_variables[dirname] = "!!!"

for unused in [
    "RULE_INPUT_PATH",
    "RULE_INPUT_ROOT",
    "RULE_INPUT_NAME",
    "RULE_INPUT_DIRNAME",
    "RULE_INPUT_EXT",
    "EXECUTABLE_PREFIX",
    "EXECUTABLE_SUFFIX",
    "STATIC_LIB_PREFIX",
    "STATIC_LIB_SUFFIX",
    "SHARED_LIB_PREFIX",
    "SHARED_LIB_SUFFIX",
    "CONFIGURATION_NAME",
]:
    generator_default_variables[unused] = ""


def _ToGypPath(path):
    """Converts a path to the format used by gyp."""
    if os.sep == Path("\\") and os.altsep == "/":
        return path.replace(Path("\\"), "/")
    return path


def _ResolveParent(path, base_path_components):
    """Resolves |path|, which starts with at least one '../'. Returns an empty
    string if the path shouldn't be considered. See _AddSources() for a
    description of |base_path_components|."""
    depth = 0
    while path.startswith("../"):
        depth += 1
        path = path[3:]
    # Relative includes may go outside the source tree. For example, an action may
    # have inputs in /usr/include, which are not in the source tree.
    if depth > len(base_path_components):
        return ""
    if depth == len(base_path_components):
        return path
    return (
        "/".join(base_path_components[0 : len(base_path_components) - depth])
        + "/"
        + path
    )


def _AddSources(sources, base_path, base_path_components, result):
    """Extracts valid sources from |sources| and adds them to |result|. Each
    source file is relative to |base_path|, but may contain '..'. To make
    resolving '..' easier |base_path_components| contains each of the
    directories in |base_path|. Additionally each source may contain variables.
    Such sources are ignored as it is assumed dependencies on them are expressed
    and tracked in some other means."""
    # NOTE: gyp paths are always posix style.
    for source in sources:
        if not len(source) or source.startswith("!!!") or source.startswith("$"):
            continue
        # variable expansion may lead to //.
        org_source = source
        source = source[0] + source[1:].replace(Path("//"), "/")
        if source.startswith("../"):
            source = _ResolveParent(source, base_path_components)
            if len(source):
                result.append(source)
            continue
        result.append(base_path + source)
        if debug:
            logger.info("AddSource", org_source, result[len(result) - 1])


def _ExtractSourcesFromAction(action, base_path, base_path_components, results):
    """_ExtractSourcesFromAction function."""

    if "inputs" in action:
        _AddSources(action["inputs"], base_path, base_path_components, results)


def _ToLocalPath(toplevel_dir, path):
    """Converts |path| to a path relative to |toplevel_dir|."""
    if path == toplevel_dir:
        return ""
    if path.startswith(toplevel_dir + "/"):
        return path[len(toplevel_dir) + len("/") :]
    return path


    """_ExtractSources function."""

def _ExtractSources(target, target_dict, toplevel_dir):
    # |target| is either absolute or relative and in the format of the OS. Gyp
    # source paths are always posix. Convert |target| to a posix path relative to
    # |toplevel_dir_|. This is done to make it easy to build source paths.
    base_path = posixpath.dirname(_ToLocalPath(toplevel_dir, _ToGypPath(target)))
    base_path_components = base_path.split("/")

    # Add a trailing '/' so that _AddSources() can easily build paths.
    if len(base_path):
        base_path += "/"

    if debug:
        logger.info("ExtractSources", target, base_path)

    results = []
    if "sources" in target_dict:
        _AddSources(target_dict["sources"], base_path, base_path_components, results)
    # Include the inputs from any actions. Any changes to these affect the
    # resulting output.
    if "actions" in target_dict:
        for action in target_dict["actions"]:
            _ExtractSourcesFromAction(action, base_path, base_path_components, results)
    if "rules" in target_dict:
        for rule in target_dict["rules"]:
            _ExtractSourcesFromAction(rule, base_path, base_path_components, results)

    return results


class Target:
    """Holds information about a particular target:
    deps: set of Targets this Target depends upon. This is not recursive, only the
      direct dependent Targets.
    match_status: one of the MatchStatus values.
    back_deps: set of Targets that have a dependency on this Target.
    visited: used during iteration to indicate whether we've visited this target.
      This is used for two iterations, once in building the set of Targets and
      again in _GetBuildTargets().
    name: fully qualified name of the target.
    requires_build: True if the target type is such that it needs to be built.
      See _DoesTargetTypeRequireBuild for details.
    added_to_compile_targets: used when determining if the target was added to the
      set of targets that needs to be built.
    in_roots: true if this target is a descendant of one of the root nodes.
    is_executable: true if the type of target is executable.
    is_static_library: true if the type of target is static_library.
    is_or_has_linked_ancestor: true if the target does a link (eg executable), or
      if there is a target in back_deps that does a link."""
        """__init__ function."""


    def __init__(self, name):
        self.deps = set()
        self.match_status = MATCH_STATUS_TBD
        self.back_deps = set()
        self.name = name
        # TODO(sky): I don't like hanging this off Target. This state is specific
        # to certain functions and should be isolated there.
        self.visited = False
        self.requires_build = False
        self.added_to_compile_targets = False
        self.in_roots = False
        self.is_executable = False
        self.is_static_library = False
        self.is_or_has_linked_ancestor = False


class Config:
    """Details what we're looking for
    files: set of files to search for
        """__init__ function."""

    targets: see file description for details."""

    def __init__(self):
        self.files = []
        self.targets = set()
        self.additional_compile_target_names = set()
        self.test_target_names = set()

    def Init(self, params):
        """Initializes Config. This is a separate method as it raises an exception
        if there is a parse error."""
        generator_flags = params.get("generator_flags", {})
        config_path = generator_flags.get("config_path", None)
        if not config_path:
            return
        try:
            f = open(config_path)
            config = json.load(f)
            f.close()
        except OSError:
            raise Exception("Unable to open file " + config_path)
        except ValueError as e:
            raise Exception("Unable to parse config file " + config_path + str(e))
        if not isinstance(config, dict):
            raise Exception("config_path must be a JSON file containing a dictionary")
        self.files = config.get("files", [])
        self.additional_compile_target_names = set(
            config.get("additional_compile_targets", [])
        )
        self.test_target_names = set(config.get("test_targets", []))


def _WasBuildFileModified(build_file, data, files, toplevel_dir):
    """Returns true if the build file |build_file| is either in |files| or
    one of the files included by |build_file| is in |files|. |toplevel_dir| is
    the root of the source tree."""
    if _ToLocalPath(toplevel_dir, _ToGypPath(build_file)) in files:
        if debug:
            logger.info("gyp file modified", build_file)
        return True

    # First element of included_files is the file itself.
    if len(data[build_file]["included_files"]) <= 1:
        return False

    for include_file in data[build_file]["included_files"][1:]:
        # |included_files| are relative to the directory of the |build_file|.
        rel_include_file = _ToGypPath(
            gyp.common.UnrelativePath(include_file, build_file)
        )
        if _ToLocalPath(toplevel_dir, rel_include_file) in files:
            if debug:
                print(
                    "included gyp file modified, gyp_file=",
                    build_file,
                    "included file=",
                    rel_include_file,
                )
            return True
    return False


def _GetOrCreateTargetByName(targets, target_name):
    """Creates or returns the Target at targets[target_name]. If there is no
    Target for |target_name| one is created. Returns a tuple of whether a new
    Target was created and the Target."""
    if target_name in targets:
        return False, targets[target_name]
    target = Target(target_name)
    targets[target_name] = target
    return True, target


def _DoesTargetTypeRequireBuild(target_dict):
    """Returns true if the target type is such that it needs to be built."""
    # If a 'none' target has rules or actions we assume it requires a build.
    return bool(
        target_dict["type"] != "none"
        or target_dict.get("actions")
        or target_dict.get("rules")
    )


def _GenerateTargets(data, target_list, target_dicts, toplevel_dir, files, build_files):
    """Returns a tuple of the following:
    . A dictionary mapping from fully qualified name to Target.
    . A list of the targets that have a source file in |files|.
    . Targets that constitute the 'all' target. See description at top of file
      for details on the 'all' target.
    This sets the |match_status| of the targets that contain any of the source
    files in |files| to MATCH_STATUS_MATCHES.
    |toplevel_dir| is the root of the source tree."""
    # Maps from target name to Target.
    name_to_target = {}

    # Targets that matched.
    matching_targets = []

    # Queue of targets to visit.
    targets_to_visit = target_list[:]

    # Maps from build file to a boolean indicating whether the build file is in
    # |files|.
    build_file_in_files = {}

    # Root targets across all files.
    roots = set()

    # Set of Targets in |build_files|.
    build_file_targets = set()

    while len(targets_to_visit) > 0:
        target_name = targets_to_visit.pop()
        created_target, target = _GetOrCreateTargetByName(name_to_target, target_name)
        if created_target:
            roots.add(target)
        elif target.visited:
            continue

        target.visited = True
        target.requires_build = _DoesTargetTypeRequireBuild(target_dicts[target_name])
        target_type = target_dicts[target_name]["type"]
        target.is_executable = target_type == "executable"
        target.is_static_library = target_type == "static_library"
        target.is_or_has_linked_ancestor = target_type in {
            "executable",
            "shared_library",
        }

        build_file = gyp.common.ParseQualifiedTarget(target_name)[0]
        if build_file not in build_file_in_files:
            build_file_in_files[build_file] = _WasBuildFileModified(
                build_file, data, files, toplevel_dir
            )

        if build_file in build_files:
            build_file_targets.add(target)

        # If a build file (or any of its included files) is modified we assume all
        # targets in the file are modified.
        if build_file_in_files[build_file]:
            logger.info("matching target from modified build file", target_name)
            target.match_status = MATCH_STATUS_MATCHES
            matching_targets.append(target)
        else:
            sources = _ExtractSources(
                target_name, target_dicts[target_name], toplevel_dir
            )
            for source in sources:
                if _ToGypPath(os.path.normpath(source)) in files:
                    logger.info("target", target_name, "matches", source)
                    target.match_status = MATCH_STATUS_MATCHES
                    matching_targets.append(target)
                    break

        # Add dependencies to visit as well as updating back pointers for deps.
        for dep in target_dicts[target_name].get("dependencies", []):
            targets_to_visit.append(dep)

            created_dep_target, dep_target = _GetOrCreateTargetByName(
                name_to_target, dep
            )
            if not created_dep_target:
                roots.discard(dep_target)

            target.deps.add(dep_target)
            dep_target.back_deps.add(target)

    return name_to_target, matching_targets, roots & build_file_targets


def _GetUnqualifiedToTargetMapping(all_targets, to_find):
    """Returns a tuple of the following:
    . mapping (dictionary) from unqualified name to Target for all the
      Targets in |to_find|.
    . any target names not found. If this is empty all targets were found."""
    result = {}
    if not to_find:
        return {}, []
    to_find = set(to_find)
    for target_name in all_targets:
        extracted = gyp.common.ParseQualifiedTarget(target_name)
        if len(extracted) > 1 and extracted[1] in to_find:
            to_find.remove(extracted[1])
            result[extracted[1]] = all_targets[target_name]
            if not to_find:
                return result, []
    return result, list(to_find)


def _DoesTargetDependOnMatchingTargets(target):
    """Returns true if |target| or any of its dependencies is one of the
    targets containing the files supplied as input to analyzer. This updates
    |matches| of the Targets as it recurses.
    target: the Target to look for."""
    if target.match_status == MATCH_STATUS_DOESNT_MATCH:
        return False
    if target.match_status in {
        MATCH_STATUS_MATCHES,
        MATCH_STATUS_MATCHES_BY_DEPENDENCY,
    }:
        return True
    for dep in target.deps:
        if _DoesTargetDependOnMatchingTargets(dep):
            target.match_status = MATCH_STATUS_MATCHES_BY_DEPENDENCY
            logger.info(Path("\t"), target.name, "matches by dep", dep.name)
            return True
    target.match_status = MATCH_STATUS_DOESNT_MATCH
    return False


def _GetTargetsDependingOnMatchingTargets(possible_targets):
    """Returns the list of Targets in |possible_targets| that depend (either
    directly on indirectly) on at least one of the targets containing the files
    supplied as input to analyzer.
    possible_targets: targets to search from."""
    found = []
    logger.info("Targets that matched by dependency:")
    for target in possible_targets:
        if _DoesTargetDependOnMatchingTargets(target):
            found.append(target)
    return found


def _AddCompileTargets(target, roots, add_if_no_ancestor, result):
    """Recurses through all targets that depend on |target|, adding all targets
    that need to be built (and are in |roots|) to |result|.
    roots: set of root targets.
    add_if_no_ancestor: If true and there are no ancestors of |target| then add
    |target| to |result|. |target| must still be in |roots|.
    result: targets that need to be built are added here."""
    if target.visited:
        return

    target.visited = True
    target.in_roots = target in roots

    for back_dep_target in target.back_deps:
        _AddCompileTargets(back_dep_target, roots, False, result)
        target.added_to_compile_targets |= back_dep_target.added_to_compile_targets
        target.in_roots |= back_dep_target.in_roots
        target.is_or_has_linked_ancestor |= back_dep_target.is_or_has_linked_ancestor

    # Always add 'executable' targets. Even though they may be built by other
    # targets that depend upon them it makes detection of what is going to be
    # built easier.
    # And always add static_libraries that have no dependencies on them from
    # linkables. This is necessary as the other dependencies on them may be
    # static libraries themselves, which are not compile time dependencies.
    if target.in_roots and (
        target.is_executable
        or (
            not target.added_to_compile_targets
            and (add_if_no_ancestor or target.requires_build)
        )
        or (
            target.is_static_library
            and add_if_no_ancestor
            and not target.is_or_has_linked_ancestor
        )
    ):
        print(
            "\t\tadding to compile targets",
            target.name,
            "executable",
            target.is_executable,
            "added_to_compile_targets",
            target.added_to_compile_targets,
            "add_if_no_ancestor",
            add_if_no_ancestor,
            "requires_build",
            target.requires_build,
            "is_static_library",
            target.is_static_library,
            "is_or_has_linked_ancestor",
            target.is_or_has_linked_ancestor,
        )
        result.add(target)
        target.added_to_compile_targets = True


def _GetCompileTargets(matching_targets, supplied_targets):
    """Returns the set of Targets that require a build.
    matching_targets: targets that changed and need to be built.
    supplied_targets: set of targets supplied to analyzer to search from."""
    result = set()
    for target in matching_targets:
        logger.info("finding compile targets for match", target.name)
        _AddCompileTargets(target, supplied_targets, True, result)
    return result


def _WriteOutput(params, **values):
    """Writes the output, either to stdout or a file is specified."""
    if "error" in values:
        logger.info("Error:", values["error"])
    if "status" in values:
        logger.info(values["status"])
    if "targets" in values:
        values["targets"].sort()
        logger.info("Supplied targets that depend on changed files:")
        for target in values["targets"]:
            logger.info(Path("\t"), target)
    if "invalid_targets" in values:
        values["invalid_targets"].sort()
        logger.info("The following targets were not found:")
        for target in values["invalid_targets"]:
            logger.info(Path("\t"), target)
    if "build_targets" in values:
        values["build_targets"].sort()
        logger.info("Targets that require a build:")
        for target in values["build_targets"]:
            logger.info(Path("\t"), target)
    if "compile_targets" in values:
        values["compile_targets"].sort()
        logger.info("Targets that need to be built:")
        for target in values["compile_targets"]:
            logger.info(Path("\t"), target)
    if "test_targets" in values:
        values["test_targets"].sort()
        logger.info("Test targets:")
        for target in values["test_targets"]:
            logger.info(Path("\t"), target)

    output_path = params.get("generator_flags", {}).get("analyzer_output_path", None)
    if not output_path:
        logger.info(json.dumps(values))
        return
    try:
        f = open(output_path, "w")
        f.write(json.dumps(values) + Path("\n"))
        f.close()
    except OSError as e:
        logger.info("Error writing to output file", output_path, str(e))


def _WasGypIncludeFileModified(params, files):
    """Returns true if one of the files in |files| is in the set of included
    files."""
    if params["options"].includes:
        for include in params["options"].includes:
            if _ToGypPath(os.path.normpath(include)) in files:
                logger.info("Include file modified, assuming all changed", include)
                return True
    return False


def _NamesNotIn(names, mapping):
    """Returns a list of the values in |names| that are not in |mapping|."""
    return [name for name in names if name not in mapping]


def _LookupTargets(names, mapping):
    """Returns a list of the mapping[name] for each value in |names| that is in
    |mapping|."""
    return [mapping[name] for name in names if name in mapping]


def CalculateVariables(default_variables, params):
    """Calculate additional variables for use in the build (called by gyp)."""
    flavor = gyp.common.GetFlavor(params)
    if flavor == "mac":
        default_variables.setdefault("OS", "mac")
    elif flavor == "win":
        default_variables.setdefault("OS", "win")
        gyp.msvs_emulation.CalculateCommonVariables(default_variables, params)
    else:
        operating_system = flavor
        if flavor == "android":
            operating_system = "linux"  # Keep this legacy behavior for now.
        default_variables.setdefault("OS", operating_system)


        """__init__ function."""

class TargetCalculator:
    """Calculates the matching test_targets and matching compile_targets."""

    def __init__(
        self,
        files,
        additional_compile_target_names,
        test_target_names,
        data,
        target_list,
        target_dicts,
        toplevel_dir,
        build_files,
    ):
        self._additional_compile_target_names = set(additional_compile_target_names)
        self._test_target_names = set(test_target_names)
        (
            self._name_to_target,
            self._changed_targets,
            self._root_targets,
        ) = _GenerateTargets(
            data, target_list, target_dicts, toplevel_dir, frozenset(files), build_files
        )
        (
            self._unqualified_mapping,
            self.invalid_targets,
        """_supplied_target_names function."""

        ) = _GetUnqualifiedToTargetMapping(
            self._name_to_target, self._supplied_target_names_no_all()
        )

    def _supplied_target_names(self):
        return self._additional_compile_target_names | self._test_target_names

    def _supplied_target_names_no_all(self):
        """Returns the supplied test targets without 'all'."""
        result = self._supplied_target_names()
        result.discard("all")
        return result

    def is_build_impacted(self):
        """Returns true if the supplied files impact the build at all."""
        return self._changed_targets

    def find_matching_test_target_names(self):
        """Returns the set of output test targets."""
        assert self.is_build_impacted()
        # Find the test targets first. 'all' is special cased to mean all the
        # root targets. To deal with all the supplied |test_targets| are expanded
        # to include the root targets during lookup. If any of the root targets
        # match, we remove it and replace it with 'all'.
        test_target_names_no_all = set(self._test_target_names)
        test_target_names_no_all.discard("all")
        test_targets_no_all = _LookupTargets(
            test_target_names_no_all, self._unqualified_mapping
        )
        test_target_names_contains_all = "all" in self._test_target_names
        if test_target_names_contains_all:
            test_targets = list(set(test_targets_no_all) | set(self._root_targets))
        else:
            test_targets = list(test_targets_no_all)
        logger.info("supplied test_targets")
        for target_name in self._test_target_names:
            logger.info(Path("\t"), target_name)
        logger.info("found test_targets")
        for target in test_targets:
            logger.info(Path("\t"), target.name)
        logger.info("searching for matching test targets")
        matching_test_targets = _GetTargetsDependingOnMatchingTargets(test_targets)
        matching_test_targets_contains_all = test_target_names_contains_all and set(
            matching_test_targets
        ) & set(self._root_targets)
        if matching_test_targets_contains_all:
            # Remove any of the targets for all that were not explicitly supplied,
            # 'all' is subsequentely added to the matching names below.
            matching_test_targets = list(
                set(matching_test_targets) & set(test_targets_no_all)
            )
        logger.info("matched test_targets")
        for target in matching_test_targets:
            logger.info(Path("\t"), target.name)
        matching_target_names = [
            gyp.common.ParseQualifiedTarget(target.name)[1]
            for target in matching_test_targets
        ]
        if matching_test_targets_contains_all:
            matching_target_names.append("all")
            logger.info(Path("\tall"))
        return matching_target_names

    def find_matching_compile_target_names(self):
        """Returns the set of output compile targets."""
        assert self.is_build_impacted()
        # Compile targets are found by searching up from changed targets.
        # Reset the visited status for _GetBuildTargets.
        for target in self._name_to_target.values():
            target.visited = False

        supplied_targets = _LookupTargets(
            self._supplied_target_names_no_all(), self._unqualified_mapping
        )
        if "all" in self._supplied_target_names():
            supplied_targets = list(set(supplied_targets) | set(self._root_targets))
        logger.info("Supplied test_targets & compile_targets")
        for target in supplied_targets:
            logger.info(Path("\t"), target.name)
        logger.info("Finding compile targets")
        compile_targets = _GetCompileTargets(self._changed_targets, supplied_targets)
        return [
            gyp.common.ParseQualifiedTarget(target.name)[1]
            for target in compile_targets
        ]


def GenerateOutput(target_list, target_dicts, data, params):
    """Called by gyp as the final stage. Outputs results."""
    config = Config()
    try:
        config.Init(params)

        if not config.files:
            raise Exception(
                "Must specify files to analyze via config_path generator " "flag"
            )

        toplevel_dir = _ToGypPath(os.path.abspath(params["options"].toplevel_dir))
        if debug:
            logger.info("toplevel_dir", toplevel_dir)

        if _WasGypIncludeFileModified(params, config.files):
            result_dict = {
                "status": all_changed_string,
                "test_targets": list(config.test_target_names),
                "compile_targets": list(
                    config.additional_compile_target_names | config.test_target_names
                ),
            }
            _WriteOutput(params, **result_dict)
            return

        calculator = TargetCalculator(
            config.files,
            config.additional_compile_target_names,
            config.test_target_names,
            data,
            target_list,
            target_dicts,
            toplevel_dir,
            params["build_files"],
        )
        if not calculator.is_build_impacted():
            result_dict = {
                "status": no_dependency_string,
                "test_targets": [],
                "compile_targets": [],
            }
            if calculator.invalid_targets:
                result_dict["invalid_targets"] = calculator.invalid_targets
            _WriteOutput(params, **result_dict)
            return

        test_target_names = calculator.find_matching_test_target_names()
        compile_target_names = calculator.find_matching_compile_target_names()
        found_at_least_one_target = compile_target_names or test_target_names
        result_dict = {
            "test_targets": test_target_names,
            "status": (
                found_dependency_string
                if found_at_least_one_target
                else no_dependency_string
            ),
            "compile_targets": list(set(compile_target_names) | set(test_target_names)),
        }
        if calculator.invalid_targets:
            result_dict["invalid_targets"] = calculator.invalid_targets
        _WriteOutput(params, **result_dict)

    except Exception as e:
        _WriteOutput(params, error=str(e))


# From consolidate_duplicates.py
    def _clean_filename_for_grouping(self, filename: str) -> str:
        """Clean filename for grouping similar files."""
        # Remove common suffixes and variations
        clean_name = filename.lower()
        
        # Remove file extensions
        clean_name = re.sub(r'\.(py|sh|md|txt|json)$', '', clean_name)
        
        # Remove common patterns
        patterns_to_remove = [
            r'\(\d+\)',  # (1), (2), etc.
            r'_\d+$',    # _1, _2, etc.
            r'\s+\d+$',  # space + number
            r'copy',     # copy
            r'backup',   # backup
            r'variants', # variants
            r'merged',   # merged
            r'consolidated', # consolidated
            r'advanced', # advanced
            r'final',    # final
            r'clean',    # clean
            r'duplicates', # duplicates
            r'archive',  # archive
        ]
        
        for pattern in patterns_to_remove:
            clean_name = re.sub(pattern, '', clean_name, flags=re.IGNORECASE)
        
        # Clean up extra spaces and underscores
        clean_name = re.sub(r'[_\s]+', '_', clean_name)
        clean_name = clean_name.strip('_')
        
        return clean_name if clean_name else None

# From consolidate_duplicates.py
    def create_consolidated_structure(self):
        """Create consolidated directory structure."""
        logger.info("Creating consolidated directory structure...")
        
        self.consolidated_dir.mkdir(exist_ok=True)
        self.archive_dir.mkdir(exist_ok=True)
        
        # Create subdirectories
        subdirs = ['scripts', 'core_analysis', 'transcription', 'generation', 'processing', 'web_scraping', 'organization', 'utilities']
        for subdir in subdirs:
            (self.consolidated_dir / subdir).mkdir(exist_ok=True)

# From consolidate_duplicates.py
    def consolidate_duplicates(self, duplicates: Dict[str, List[Path]]):
        """Consolidate duplicate files by keeping the best version."""
        logger.info("Consolidating duplicate files...")
        
        consolidated_count = 0
        archived_count = 0
        
        for file_hash, files in duplicates.items():
            if len(files) > 1:
                # Select best file
                best_file = self.select_best_file(files)
                if not best_file:
                    continue
                
                # Determine target location
                target_dir = self._determine_target_directory(best_file)
                target_path = target_dir / best_file.name
                
                # Copy best file to consolidated location
                try:
                    shutil.copy2(best_file, target_path)
                    consolidated_count += 1
                    logger.info(f"Consolidated: {best_file.name}")
                except Exception as e:
                    logger.error(f"Error copying {best_file}: {e}")
                    continue
                
                # Archive other files
                for file_path in files:
                    if file_path != best_file:
                        try:
                            archive_path = self.archive_dir / file_path.name
                            # Handle duplicate names in archive
                            counter = 1
                            original_archive = archive_path
                            while archive_path.exists():
                                stem = original_archive.stem
                                suffix = original_archive.suffix
                                archive_path = original_archive.parent / f"{stem}_{counter}{suffix}"
                                counter += 1
                            
                            shutil.move(str(file_path), str(archive_path))
                            archived_count += 1
                            logger.info(f"Archived: {file_path.name}")
                        except Exception as e:
                            logger.error(f"Error archiving {file_path}: {e}")
        
        logger.info(f"Consolidated {consolidated_count} files, archived {archived_count} duplicates")

# From consolidate_duplicates.py
    def consolidate_similar_files(self, similar_groups: Dict[str, List[Path]]):
        """Consolidate similar files by keeping the best version."""
        logger.info("Consolidating similar files...")
        
        consolidated_count = 0
        archived_count = 0
        
        for group_name, files in similar_groups.items():
            if len(files) > 1:
                # Select best file
                best_file = self.select_best_file(files)
                if not best_file:
                    continue
                
                # Determine target location
                target_dir = self._determine_target_directory(best_file)
                target_path = target_dir / best_file.name
                
                # Copy best file to consolidated location
                try:
                    shutil.copy2(best_file, target_path)
                    consolidated_count += 1
                    logger.info(f"Consolidated similar: {best_file.name}")
                except Exception as e:
                    logger.error(f"Error copying {best_file}: {e}")
                    continue
                
                # Archive other files
                for file_path in files:
                    if file_path != best_file:
                        try:
                            archive_path = self.archive_dir / f"{group_name}_{file_path.name}"
                            shutil.move(str(file_path), str(archive_path))
                            archived_count += 1
                            logger.info(f"Archived similar: {file_path.name}")
                        except Exception as e:
                            logger.error(f"Error archiving {file_path}: {e}")
        
        logger.info(f"Consolidated {consolidated_count} similar files, archived {archived_count} duplicates")

# From consolidate_duplicates.py
    def _determine_target_directory(self, file_path: Path) -> Path:
        """Determine target directory based on file content and name."""
        analysis = self.content_analysis.get(file_path, {})
        content = analysis.get('content', '').lower()
        filename = file_path.name.lower()
        
        # Determine category based on content and filename
        if any(keyword in content for keyword in ['analyze', 'analysis', 'analyzer']):
            return self.consolidated_dir / 'core_analysis'
        elif any(keyword in content for keyword in ['transcribe', 'transcript', 'whisper', 'speech']):
            return self.consolidated_dir / 'transcription'
        elif any(keyword in content for keyword in ['generate', 'create', 'build', 'html', 'csv']):
            return self.consolidated_dir / 'generation'
        elif any(keyword in content for keyword in ['process', 'convert', 'mp3', 'mp4', 'ffmpeg']):
            return self.consolidated_dir / 'processing'
        elif any(keyword in content for keyword in ['scrape', 'suno', 'beautifulsoup', 'requests']):
            return self.consolidated_dir / 'web_scraping'
        elif any(keyword in content for keyword in ['organize', 'sort', 'manage', 'file']):
            return self.consolidated_dir / 'organization'
        elif filename.endswith('.sh'):
            return self.consolidated_dir / 'scripts'
        else:
            return self.consolidated_dir / 'utilities'

# From consolidate_duplicates.py
    def generate_consolidation_report(self, duplicates: Dict[str, List[Path]], similar_groups: Dict[str, List[Path]]):
        """Generate comprehensive consolidation report."""
        report_path = self.consolidated_dir / "CONSOLIDATION_REPORT.md"
        
        with open(report_path, 'w') as f:
            f.write("# Duplicate Consolidation Report\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## Overview\n\n")
            f.write(f"- **Exact duplicate groups:** {len(duplicates)}\n")
            f.write(f"- **Similar file groups:** {len(similar_groups)}\n")
            f.write(f"- **Total files analyzed:** {len(self.content_analysis)}\n")
            f.write(f"- **Consolidated directory:** {self.consolidated_dir}\n")
            f.write(f"- **Archive directory:** {self.archive_dir}\n\n")
            
            f.write("## Exact Duplicate Groups\n\n")
            for file_hash, files in duplicates.items():
                f.write(f"### Group {file_hash[:8]}...\n")
                f.write(f"Files: {len(files)}\n")
                for file_path in files:
                    analysis = self.content_analysis.get(file_path, {})
                    f.write(f"- {file_path} (Quality: {analysis.get('quality_score', 0):.2f})\n")
                f.write(Path("\n"))
            
            f.write("## Similar File Groups\n\n")
            for group_name, files in similar_groups.items():
                f.write(f"### {group_name.replace('_', ' ').title()}\n")
                f.write(f"Files: {len(files)}\n")
                for file_path in files:
                    analysis = self.content_analysis.get(file_path, {})
                    f.write(f"- {file_path} (Quality: {analysis.get('quality_score', 0):.2f})\n")
                f.write(Path("\n"))
            
            f.write("## Quality Analysis Summary\n\n")
            quality_stats = defaultdict(int)
            for analysis in self.content_analysis.values():
                score_range = int(analysis.get('quality_score', 0) // 10) * 10
                quality_stats[score_range] += 1
            
            for score_range, count in sorted(quality_stats.items()):
                f.write(f"- Quality Score {score_range}-{score_range+9}: {count} files\n")
        
        logger.info(f"Consolidation report generated: {report_path}")

# From consolidate_duplicates.py
    def run_consolidation(self):
        """Run the complete consolidation process."""
        logger.info("Starting duplicate consolidation process...")
        
        try:
            # Step 1: Find exact duplicates
            duplicates = self.find_exact_duplicates()
            
            # Step 2: Find similar files
            similar_groups = self.find_similar_files()
            
            # Step 3: Create consolidated structure
            self.create_consolidated_structure()
            
            # Step 4: Consolidate duplicates
            self.consolidate_duplicates(duplicates)
            
            # Step 5: Consolidate similar files
            self.consolidate_similar_files(similar_groups)
            
            # Step 6: Generate report
            self.generate_consolidation_report(duplicates, similar_groups)
            
            logger.info("Duplicate consolidation process completed!")
            
            # Print summary
            total_duplicates = sum(len(files) - 1 for files in duplicates.values())
            total_similar = sum(len(files) - 1 for files in similar_groups.values())
            
            logger.info(f"\n Duplicate Consolidation Complete!")
            logger.info(f" Exact duplicates found: {len(duplicates)} groups ({total_duplicates} files)")
            logger.info(f" Similar files found: {len(similar_groups)} groups ({total_similar} files)")
            logger.info(f" Consolidated directory: {self.consolidated_dir}")
            logger.info(f" Archive directory: {self.archive_dir}")
            logger.info(f" Report: {self.consolidated_dir}/CONSOLIDATION_REPORT.md")
            
        except Exception as e:
            logger.error(f"Error during consolidation: {e}")
            raise
        """__init__ function."""


# From consolidate_duplicates.py
class DuplicateConsolidator:
    """Consolidates duplicate files by selecting the best version and removing extras."""
    
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.consolidated_dir = self.base_dir / "CONSOLIDATED_FINAL"
        self.archive_dir = self.base_dir / "CONSOLIDATED_ARCHIVE"
        
        # File analysis cache
        self.file_hashes = {}
        self.content_analysis = {}
        self.duplicate_groups = defaultdict(list)
        self.similar_groups = defaultdict(list)
        
        # Quality scoring weights
        self.quality_weights = {
            'has_docstring': 3,
            'has_logging': 2,
            'has_error_handling': 3,
            'has_main_function': 2,
            'has_type_hints': 2,
            'has_comments': 1,
            'file_size': 0.1,  # per KB
            'line_count': 0.01,  # per line
            'function_count': 2,
            'class_count': 3,
            'import_count': 0.5
        }
    
    def calculate_file_hash(self, file_path: Path) -> str:
        """Calculate MD5 hash of file content."""
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
            return hashlib.md5(content).hexdigest()
        except Exception as e:
            logger.warning(f"Could not calculate hash for {file_path}: {e}")
            return ""
    
    def analyze_file_content(self, file_path: Path) -> Dict:
        """Analyze file content for quality scoring."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            non_empty_lines = [line for line in lines if line.strip()]
            
            # Basic metrics
            analysis = {
                'file_path': file_path,
                'filename': file_path.name,
                'size': file_path.stat().st_size,
                'lines': len(lines),
                'non_empty_lines': len(non_empty_lines),
                'content': content,
                'has_docstring': False,
                'has_logging': False,
                'has_error_handling': False,
                'has_main_function': False,
                'has_type_hints': False,
                'has_comments': False,
                'function_count': 0,
                'class_count': 0,
                'import_count': 0,
                'quality_score': 0
            }
            
            # Check for quality indicators
            content_lower = content.lower()
            analysis['has_docstring'] = '"""' in content or "'''" in content
            analysis['has_logging'] = 'logging' in content_lower or 'logger' in content_lower
            analysis['has_error_handling'] = 'try:' in content and 'except' in content
            analysis['has_main_function'] = 'if __name__ == "__main__":' in content
            analysis['has_type_hints'] = '->' in content or ': ' in content and any(t in content for t in ['str', 'int', 'List', 'Dict', 'Optional'])
            analysis['has_comments'] = any(line.strip().startswith('#') and len(line.strip()) > 3 for line in lines)
            
            # Count functions and classes
            analysis['function_count'] = len(re.findall(r'def\s+\w+', content))
            analysis['class_count'] = len(re.findall(r'class\s+\w+', content))
            analysis['import_count'] = len([line for line in lines if line.strip().startswith(('import ', 'from '))])
            
            # Calculate quality score
            score = 0
            for indicator, weight in self.quality_weights.items():
                if indicator in analysis:
                    if isinstance(analysis[indicator], bool):
                        score += weight if analysis[indicator] else 0
                    else:
                        score += analysis[indicator] * weight
            
            analysis['quality_score'] = score
            
            return analysis
            
        except Exception as e:
            logger.warning(f"Could not analyze {file_path}: {e}")
            return {
                'file_path': file_path,
                'filename': file_path.name,
                'size': 0,
                'lines': 0,
                'non_empty_lines': 0,
                'content': '',
                'has_docstring': False,
                'has_logging': False,
                'has_error_handling': False,
                'has_main_function': False,
                'has_type_hints': False,
                'has_comments': False,
                'function_count': 0,
                'class_count': 0,
                'import_count': 0,
                'quality_score': 0
            }
    
    def find_exact_duplicates(self) -> Dict[str, List[Path]]:
        """Find files with identical content."""
        logger.info("Finding exact duplicates...")
        
        for file_path in self.base_dir.rglob("*"):
            if file_path.is_file() and file_path.suffix in ['.py', '.sh', '.md', '.txt', '.json']:
                file_hash = self.calculate_file_hash(file_path)
                if file_hash:
                    self.duplicate_groups[file_hash].append(file_path)
        
        # Return only groups with duplicates
        duplicates = {h: files for h, files in self.duplicate_groups.items() if len(files) > 1}
        logger.info(f"Found {len(duplicates)} groups of exact duplicates")
        return duplicates
    
    def find_similar_files(self) -> Dict[str, List[Path]]:
        """Find files with similar names and purposes."""
        logger.info("Finding similar files...")
        
        # Group by base filename (without extensions and numbers)
        filename_groups = defaultdict(list)
        
        for file_path in self.base_dir.rglob("*"):
            if file_path.is_file() and file_path.suffix in ['.py', '.sh', '.md', '.txt', '.json']:
                # Clean filename for grouping
                clean_name = self._clean_filename_for_grouping(file_path.name)
                if clean_name:
                    filename_groups[clean_name].append(file_path)
        
        # Return only groups with multiple files
        similar_groups = {k: v for k, v in filename_groups.items() if len(v) > 1}
        logger.info(f"Found {len(similar_groups)} groups of similar files")
        return similar_groups
    
    def _clean_filename_for_grouping(self, filename: str) -> str:
        """Clean filename for grouping similar files."""
        # Remove common suffixes and variations
        clean_name = filename.lower()
        
        # Remove file extensions
        clean_name = re.sub(r'\.(py|sh|md|txt|json)$', '', clean_name)
        
        # Remove common patterns
        patterns_to_remove = [
            r'\(\d+\)',  # (1), (2), etc.
            r'_\d+$',    # _1, _2, etc.
            r'\s+\d+$',  # space + number
            r'copy',     # copy
            r'backup',   # backup
            r'variants', # variants
            r'merged',   # merged
            r'consolidated', # consolidated
            r'advanced', # advanced
            r'final',    # final
            r'clean',    # clean
            r'duplicates', # duplicates
            r'archive',  # archive
        ]
        
        for pattern in patterns_to_remove:
            clean_name = re.sub(pattern, '', clean_name, flags=re.IGNORECASE)
        
        # Clean up extra spaces and underscores
        clean_name = re.sub(r'[_\s]+', '_', clean_name)
        clean_name = clean_name.strip('_')
        
        return clean_name if clean_name else None
    
    def select_best_file(self, files: List[Path]) -> Path:
        """Select the best file from a group based on quality metrics."""
        if not files:
            return None
        
        if len(files) == 1:
            return files[0]
        
        # Analyze all files
        file_analyses = []
        for file_path in files:
            if file_path not in self.content_analysis:
                self.content_analysis[file_path] = self.analyze_file_content(file_path)
            file_analyses.append(self.content_analysis[file_path])
        
        # Sort by quality score (descending)
        file_analyses.sort(key=lambda x: x['quality_score'], reverse=True)
        
        best_file = file_analyses[0]['file_path']
        logger.info(f"Selected best file: {best_file.name} (score: {file_analyses[0]['quality_score']:.2f})")
        
        return best_file
    
    def create_consolidated_structure(self):
        """Create consolidated directory structure."""
        logger.info("Creating consolidated directory structure...")
        
        self.consolidated_dir.mkdir(exist_ok=True)
        self.archive_dir.mkdir(exist_ok=True)
        
        # Create subdirectories
        subdirs = ['scripts', 'core_analysis', 'transcription', 'generation', 'processing', 'web_scraping', 'organization', 'utilities']
        for subdir in subdirs:
            (self.consolidated_dir / subdir).mkdir(exist_ok=True)
    
    def consolidate_duplicates(self, duplicates: Dict[str, List[Path]]):
        """Consolidate duplicate files by keeping the best version."""
        logger.info("Consolidating duplicate files...")
        
        consolidated_count = 0
        archived_count = 0
        
        for file_hash, files in duplicates.items():
            if len(files) > 1:
                # Select best file
                best_file = self.select_best_file(files)
                if not best_file:
                    continue
                
                # Determine target location
                target_dir = self._determine_target_directory(best_file)
                target_path = target_dir / best_file.name
                
                # Copy best file to consolidated location
                try:
                    shutil.copy2(best_file, target_path)
                    consolidated_count += 1
                    logger.info(f"Consolidated: {best_file.name}")
                except Exception as e:
                    logger.error(f"Error copying {best_file}: {e}")
                    continue
                
                # Archive other files
                for file_path in files:
                    if file_path != best_file:
                        try:
                            archive_path = self.archive_dir / file_path.name
                            # Handle duplicate names in archive
                            counter = 1
                            original_archive = archive_path
                            while archive_path.exists():
                                stem = original_archive.stem
                                suffix = original_archive.suffix
                                archive_path = original_archive.parent / f"{stem}_{counter}{suffix}"
                                counter += 1
                            
                            shutil.move(str(file_path), str(archive_path))
                            archived_count += 1
                            logger.info(f"Archived: {file_path.name}")
                        except Exception as e:
                            logger.error(f"Error archiving {file_path}: {e}")
        
        logger.info(f"Consolidated {consolidated_count} files, archived {archived_count} duplicates")
    
    def consolidate_similar_files(self, similar_groups: Dict[str, List[Path]]):
        """Consolidate similar files by keeping the best version."""
        logger.info("Consolidating similar files...")
        
        consolidated_count = 0
        archived_count = 0
        
        for group_name, files in similar_groups.items():
            if len(files) > 1:
                # Select best file
                best_file = self.select_best_file(files)
                if not best_file:
                    continue
                
                # Determine target location
                target_dir = self._determine_target_directory(best_file)
                target_path = target_dir / best_file.name
                
                # Copy best file to consolidated location
                try:
                    shutil.copy2(best_file, target_path)
                    consolidated_count += 1
                    logger.info(f"Consolidated similar: {best_file.name}")
                except Exception as e:
                    logger.error(f"Error copying {best_file}: {e}")
                    continue
                
                # Archive other files
                for file_path in files:
                    if file_path != best_file:
                        try:
                            archive_path = self.archive_dir / f"{group_name}_{file_path.name}"
                            shutil.move(str(file_path), str(archive_path))
                            archived_count += 1
                            logger.info(f"Archived similar: {file_path.name}")
                        except Exception as e:
                            logger.error(f"Error archiving {file_path}: {e}")
        
        logger.info(f"Consolidated {consolidated_count} similar files, archived {archived_count} duplicates")
    
    def _determine_target_directory(self, file_path: Path) -> Path:
        """Determine target directory based on file content and name."""
        analysis = self.content_analysis.get(file_path, {})
        content = analysis.get('content', '').lower()
        filename = file_path.name.lower()
        
        # Determine category based on content and filename
        if any(keyword in content for keyword in ['analyze', 'analysis', 'analyzer']):
            return self.consolidated_dir / 'core_analysis'
        elif any(keyword in content for keyword in ['transcribe', 'transcript', 'whisper', 'speech']):
            return self.consolidated_dir / 'transcription'
        elif any(keyword in content for keyword in ['generate', 'create', 'build', 'html', 'csv']):
            return self.consolidated_dir / 'generation'
        elif any(keyword in content for keyword in ['process', 'convert', 'mp3', 'mp4', 'ffmpeg']):
            return self.consolidated_dir / 'processing'
        elif any(keyword in content for keyword in ['scrape', 'suno', 'beautifulsoup', 'requests']):
            return self.consolidated_dir / 'web_scraping'
        elif any(keyword in content for keyword in ['organize', 'sort', 'manage', 'file']):
            return self.consolidated_dir / 'organization'
        elif filename.endswith('.sh'):
            return self.consolidated_dir / 'scripts'
        else:
            return self.consolidated_dir / 'utilities'
    
    def generate_consolidation_report(self, duplicates: Dict[str, List[Path]], similar_groups: Dict[str, List[Path]]):
        """Generate comprehensive consolidation report."""
        report_path = self.consolidated_dir / "CONSOLIDATION_REPORT.md"
        
        with open(report_path, 'w') as f:
            f.write("# Duplicate Consolidation Report\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## Overview\n\n")
            f.write(f"- **Exact duplicate groups:** {len(duplicates)}\n")
            f.write(f"- **Similar file groups:** {len(similar_groups)}\n")
            f.write(f"- **Total files analyzed:** {len(self.content_analysis)}\n")
            f.write(f"- **Consolidated directory:** {self.consolidated_dir}\n")
            f.write(f"- **Archive directory:** {self.archive_dir}\n\n")
            
            f.write("## Exact Duplicate Groups\n\n")
            for file_hash, files in duplicates.items():
                f.write(f"### Group {file_hash[:8]}...\n")
                f.write(f"Files: {len(files)}\n")
                for file_path in files:
                    analysis = self.content_analysis.get(file_path, {})
                    f.write(f"- {file_path} (Quality: {analysis.get('quality_score', 0):.2f})\n")
                f.write(Path("\n"))
            
            f.write("## Similar File Groups\n\n")
            for group_name, files in similar_groups.items():
                f.write(f"### {group_name.replace('_', ' ').title()}\n")
                f.write(f"Files: {len(files)}\n")
                for file_path in files:
                    analysis = self.content_analysis.get(file_path, {})
                    f.write(f"- {file_path} (Quality: {analysis.get('quality_score', 0):.2f})\n")
                f.write(Path("\n"))
            
            f.write("## Quality Analysis Summary\n\n")
            quality_stats = defaultdict(int)
            for analysis in self.content_analysis.values():
                score_range = int(analysis.get('quality_score', 0) // 10) * 10
                quality_stats[score_range] += 1
            
            for score_range, count in sorted(quality_stats.items()):
                f.write(f"- Quality Score {score_range}-{score_range+9}: {count} files\n")
        
        logger.info(f"Consolidation report generated: {report_path}")
    
    def run_consolidation(self):
        """Run the complete consolidation process."""
        logger.info("Starting duplicate consolidation process...")
        
        try:
            # Step 1: Find exact duplicates
            duplicates = self.find_exact_duplicates()
            
            # Step 2: Find similar files
            similar_groups = self.find_similar_files()
            
            # Step 3: Create consolidated structure
            self.create_consolidated_structure()
            
            # Step 4: Consolidate duplicates
            self.consolidate_duplicates(duplicates)
            
            # Step 5: Consolidate similar files
            self.consolidate_similar_files(similar_groups)
            
            # Step 6: Generate report
            self.generate_consolidation_report(duplicates, similar_groups)
            
            logger.info("Duplicate consolidation process completed!")
            
            # Print summary
            total_duplicates = sum(len(files) - 1 for files in duplicates.values())
            total_similar = sum(len(files) - 1 for files in similar_groups.values())
            
            logger.info(f"\n Duplicate Consolidation Complete!")
            logger.info(f" Exact duplicates found: {len(duplicates)} groups ({total_duplicates} files)")
            logger.info(f" Similar files found: {len(similar_groups)} groups ({total_similar} files)")
            logger.info(f" Consolidated directory: {self.consolidated_dir}")
            logger.info(f" Archive directory: {self.archive_dir}")
            logger.info(f" Report: {self.consolidated_dir}/CONSOLIDATION_REPORT.md")
            
        except Exception as e:
            logger.error(f"Error during consolidation: {e}")
            raise

# From deep_analysis_merge.py
    def calculate_similarity(self, file1: Path, file2: Path) -> float:
        """Calculate similarity between two files."""
        try:
            analysis1 = self.content_analysis.get(file1)
            analysis2 = self.content_analysis.get(file2)
            
            if not analysis1 or not analysis2:
                return 0.0
            
            # Compare functionality patterns
            patterns1 = set(analysis1['functionality_patterns'])
            patterns2 = set(analysis2['functionality_patterns'])
            pattern_similarity = len(patterns1.intersection(patterns2)) / max(len(patterns1.union(patterns2)), 1)
            
            # Compare imports
            imports1 = set(analysis1['imports'])
            imports2 = set(analysis2['imports'])
            import_similarity = len(imports1.intersection(imports2)) / max(len(imports1.union(imports2)), 1)
            
            # Compare functions
            funcs1 = set(analysis1['functions'])
            funcs2 = set(analysis2['functions'])
            func_similarity = len(funcs1.intersection(funcs2)) / max(len(funcs1.union(funcs2)), 1)
            
            # Compare content using difflib
            content1 = analysis1['content']
            content2 = analysis2['content']
            content_similarity = difflib.SequenceMatcher(None, content1, content2).ratio()
            
            # Weighted average
            total_similarity = (
                pattern_similarity * 0.3 +
                import_similarity * 0.2 +
                func_similarity * 0.2 +
                content_similarity * 0.3
            )
            
            return total_similarity
            
        except Exception as e:
            logger.warning(f"Error calculating similarity between {file1} and {file2}: {e}")
            return 0.0

# From deep_analysis_merge.py
    def create_merged_file(self, files: List[Path], output_path: Path) -> bool:
        """Create a merged file combining the best features from multiple files."""
        try:
            if not files:
                return False
            
            # Select the best base file
            base_file = self.select_best_file(files)
            if not base_file:
                return False
            
            # Read base file content
            with open(base_file, 'r', encoding='utf-8') as f:
                base_content = f.read()
            
            # Analyze all files to extract unique features
            all_analyses = []
            for file_path in files:
                if file_path not in self.content_analysis:
                    self.content_analysis[file_path] = self.analyze_file_content(file_path)
                all_analyses.append(self.content_analysis[file_path])
            
            # Extract unique functions, classes, and imports
            all_functions = set()
            all_classes = set()
            all_imports = set()
            all_docstrings = set()
            
            for analysis in all_analyses:
                all_functions.update(analysis['functions'])
                all_classes.update(analysis['classes'])
                all_imports.update(analysis['imports'])
                all_docstrings.update(analysis['docstrings'])
            
            # Create merged content
            merged_content = self._create_merged_content(
                base_content, all_functions, all_classes, all_imports, all_docstrings, files
            )
            
            # Write merged file
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(merged_content)
            
            logger.info(f"Created merged file: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"Error creating merged file {output_path}: {e}")
            return False

# From deep_analysis_merge.py
    def run_deep_analysis_and_merge(self):
        """Run the complete deep analysis and merge process."""
        logger.info("Starting deep analysis and merge process...")
        
        try:
            # Step 1: Find exact duplicates
            exact_duplicates = self.find_exact_duplicates()
            
            # Step 2: Find similar files
            similar_groups = self.find_similar_files()
            
            # Step 3: Create merged directory
            self.merged_dir.mkdir(exist_ok=True)
            
            # Step 4: Process exact duplicates
            logger.info("Processing exact duplicates...")
            for hash_val, files in exact_duplicates.items():
                best_file = self.select_best_file(files)
                if best_file:
                    # Copy best file to merged directory
                    target_path = self.merged_dir / best_file.name
                    self._copy_file(best_file, target_path)
                    logger.info(f"Kept best duplicate: {best_file.name}")
            
            # Step 5: Process similar files
            logger.info("Processing similar files...")
            for group_name, files in similar_groups.items():
                if len(files) > 1:
                    # Create merged file
                    merged_filename = self._generate_merged_filename(files)
                    merged_path = self.merged_dir / merged_filename
                    
                    if self.create_merged_file(files, merged_path):
                        logger.info(f"Created merged file: {merged_filename} from {len(files)} files")
                    else:
                        # Fallback: just copy the best file
                        best_file = self.select_best_file(files)
                        if best_file:
                            target_path = self.merged_dir / best_file.name
                            self._copy_file(best_file, target_path)
                            logger.info(f"Kept best file: {best_file.name}")
            
            # Step 6: Generate analysis report
            self._generate_analysis_report(exact_duplicates, similar_groups)
            
            logger.info("Deep analysis and merge process completed!")
            
        except Exception as e:
            logger.error(f"Error during deep analysis and merge: {e}")
            raise

# From deep_analysis_merge.py
    def _copy_file(self, src: Path, dst: Path):
        """Copy file from source to destination."""
        try:
            import shutil
            dst.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dst)
        except Exception as e:
            logger.error(f"Error copying {src} to {dst}: {e}")

# From deep_analysis_merge.py
    def _generate_merged_filename(self, files: List[Path]) -> str:
        """Generate a filename for merged files."""
        # Get common base name
        base_names = [f.stem for f in files]
        common_prefix = os.path.commonprefix(base_names)
        
        if common_prefix:
            return f"{common_prefix}_merged.py"
        else:
            return f"merged_{len(files)}_files.py"

# From deep_analysis_merge.py
    def _generate_analysis_report(self, exact_duplicates: Dict[str, List[Path]], 
                                similar_groups: Dict[str, List[Path]]):
        """Generate comprehensive analysis report."""
        report_path = self.merged_dir / "DEEP_ANALYSIS_REPORT.md"
        
        with open(report_path, 'w') as f:
            f.write("# Deep Analysis and Merge Report\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## Overview\n\n")
            f.write(f"- **Exact duplicate groups:** {len(exact_duplicates)}\n")
            f.write(f"- **Similar file groups:** {len(similar_groups)}\n")
            f.write(f"- **Total files analyzed:** {len(self.content_analysis)}\n\n")
            
            f.write("## Exact Duplicates Found\n\n")
            for hash_val, files in exact_duplicates.items():
                f.write(f"### Group {hash_val[:8]}...\n")
                f.write(f"Files: {len(files)}\n")
                for file_path in files:
                    f.write(f"- {file_path}\n")
                f.write(Path("\n"))
            
            f.write("## Similar File Groups\n\n")
            for group_name, files in similar_groups.items():
                f.write(f"### {group_name}\n")
                f.write(f"Files: {len(files)}\n")
                for file_path in files:
                    analysis = self.content_analysis.get(file_path, {})
                    f.write(f"- {file_path} (Quality: {analysis.get('quality_score', 0)})\n")
                f.write(Path("\n"))
            
            f.write("## Quality Analysis Summary\n\n")
            quality_stats = defaultdict(int)
            for analysis in self.content_analysis.values():
                quality_stats[analysis.get('quality_score', 0)] += 1
            
            for score, count in sorted(quality_stats.items()):
                f.write(f"- Quality Score {score}: {count} files\n")
            
            f.write("\n## Functionality Patterns\n\n")
            pattern_stats = defaultdict(int)
            for analysis in self.content_analysis.values():
                for pattern in analysis.get('functionality_patterns', []):
                    pattern_stats[pattern] += 1
            
            for pattern, count in sorted(pattern_stats.items(), key=lambda x: x[1], reverse=True):
        """__init__ function."""

                f.write(f"- {pattern}: {count} files\n")

# From deep_analysis_merge.py
class DeepAnalysisMerger:
    """Deep analysis and intelligent merge tool for Python files."""
    
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.final_dir = self.base_dir / "FINAL_ORGANIZED"
        self.duplicates_dir = self.base_dir / "DUPLICATES_ARCHIVE"
        self.clean_dir = self.base_dir / "CLEAN_ORGANIZED"
        self.merged_dir = self.base_dir / "DEEP_MERGED"
        
        # Analysis results
        self.file_hashes = {}
        self.content_analysis = {}
        self.similarity_groups = defaultdict(list)
        self.merge_opportunities = []
        self.quality_scores = {}
        
        # File patterns for analysis
        self.analysis_patterns = {
            'analyze': r'analyze|analysis|analyzer',
            'generate': r'generate|generation|generator',
            'transcribe': r'transcribe|transcript|transcription',
            'process': r'process|processing|processor',
            'organize': r'organize|organization|organizer',
            'scrape': r'scrape|scraping|scraper',
            'convert': r'convert|conversion|converter',
            'mp3': r'mp3|audio|sound',
            'mp4': r'mp4|video|visual',
            'csv': r'csv|data|spreadsheet',
            'html': r'html|web|page'
        }

    def calculate_file_hash(self, file_path: Path) -> str:
        """Calculate MD5 hash of file content."""
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
            return hashlib.md5(content).hexdigest()
        except Exception as e:
            logger.warning(f"Could not calculate hash for {file_path}: {e}")
            return ""

    def analyze_file_content(self, file_path: Path) -> Dict[str, Any]:
        """Deep analysis of file content."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            non_empty_lines = [line for line in lines if line.strip()]
            
            # Basic metrics
            analysis = {
                'file_path': file_path,
                'filename': file_path.name,
                'size': file_path.stat().st_size,
                'lines': len(lines),
                'non_empty_lines': len(non_empty_lines),
                'content': content,
                'imports': [],
                'functions': [],
                'classes': [],
                'docstrings': [],
                'openai_usage': False,
                'quality_indicators': {
                    'has_docstring': False,
                    'has_logging': False,
                    'has_error_handling': False,
                    'has_main_function': False,
                    'has_type_hints': False,
                    'has_comments': False
                },
                'functionality_patterns': [],
                'dependencies': set(),
                'complexity_score': 0
            }
            
            # Extract imports
            for line in lines:
                if line.strip().startswith('import ') or line.strip().startswith('from '):
                    analysis['imports'].append(line.strip())
                    # Extract module names
                    if 'import ' in line:
                        module = line.split('import ')[1].split(' as ')[0].split('.')[0].strip()
                        analysis['dependencies'].add(module)
                    elif 'from ' in line:
                        module = line.split('from ')[1].split(' import')[0].split('.')[0].strip()
                        analysis['dependencies'].add(module)
            
            # Extract functions and classes
            in_function = False
            in_class = False
            current_function = ""
            current_class = ""
            
            for i, line in enumerate(lines):
                stripped = line.strip()
                
                # Check for docstrings
                if '"""' in line or "'''" in line:
                    analysis['quality_indicators']['has_docstring'] = True
                    if '"""' in line:
                        analysis['docstrings'].append(line)
                
                # Check for logging
                if 'logging' in line.lower() or 'logger' in line.lower():
                    analysis['quality_indicators']['has_logging'] = True
                
                # Check for error handling
                if 'try:' in line or 'except' in line or 'finally:' in line:
                    analysis['quality_indicators']['has_error_handling'] = True
                
                # Check for main function
                if 'if __name__ == "__main__":' in line:
                    analysis['quality_indicators']['has_main_function'] = True
                
                # Check for type hints
                if '->' in line or ': ' in line and ('str' in line or 'int' in line or 'List' in line or 'Dict' in line):
                    analysis['quality_indicators']['has_type_hints'] = True
                
                # Check for comments
                if line.strip().startswith('#') and len(line.strip()) > 3:
                    analysis['quality_indicators']['has_comments'] = True
                
                # Extract functions
                if re.match(r'def\s+\w+', stripped):
                    func_name = re.match(r'def\s+(\w+)', stripped).group(1)
                    analysis['functions'].append(func_name)
                    in_function = True
                    current_function = func_name
                
                # Extract classes
                if re.match(r'class\s+\w+', stripped):
                    class_name = re.match(r'class\s+(\w+)', stripped).group(1)
                    analysis['classes'].append(class_name)
                    in_class = True
                    current_class = class_name
                
                # Check for OpenAI usage
                if 'openai' in line.lower() or 'gpt' in line.lower() or 'whisper' in line.lower():
                    analysis['openai_usage'] = True
            
            # Identify functionality patterns
            content_lower = content.lower()
            for pattern_name, pattern in self.analysis_patterns.items():
                if re.search(pattern, content_lower):
                    analysis['functionality_patterns'].append(pattern_name)
            
            # Calculate quality score
            quality_score = 0
            for indicator, has_it in analysis['quality_indicators'].items():
                if has_it:
                    quality_score += 1
            
            # Add complexity score based on lines, functions, classes
            analysis['complexity_score'] = len(analysis['functions']) + len(analysis['classes']) + (len(non_empty_lines) // 10)
            analysis['quality_score'] = quality_score
            
            return analysis
            
        except Exception as e:
            logger.warning(f"Could not analyze {file_path}: {e}")
            return {
                'file_path': file_path,
                'filename': file_path.name,
                'size': 0,
                'lines': 0,
                'non_empty_lines': 0,
                'content': '',
                'imports': [],
                'functions': [],
                'classes': [],
                'docstrings': [],
                'openai_usage': False,
                'quality_indicators': {},
                'functionality_patterns': [],
                'dependencies': set(),
                'complexity_score': 0,
                'quality_score': 0
            }

    def find_exact_duplicates(self) -> Dict[str, List[Path]]:
        """Find exact duplicate files by content hash."""
        logger.info("Finding exact duplicates...")
        
        hash_to_files = defaultdict(list)
        
        # Scan all Python files
        for py_file in self.base_dir.rglob("*.py"):
            if py_file.exists():
                file_hash = self.calculate_file_hash(py_file)
                if file_hash:
                    hash_to_files[file_hash].append(py_file)
        
        # Filter out single files (no duplicates)
        duplicates = {h: files for h, files in hash_to_files.items() if len(files) > 1}
        
        logger.info(f"Found {len(duplicates)} groups of exact duplicates")
        return duplicates

    def find_similar_files(self) -> Dict[str, List[Path]]:
        """Find similar files based on content analysis."""
        logger.info("Finding similar files...")
        
        # Analyze all Python files
        all_files = []
        for py_file in self.base_dir.rglob("*.py"):
            if py_file.exists():
                analysis = self.analyze_file_content(py_file)
                all_files.append(analysis)
                self.content_analysis[py_file] = analysis
        
        # Group by functionality patterns
        pattern_groups = defaultdict(list)
        for analysis in all_files:
            if analysis['functionality_patterns']:
                # Create a signature based on functionality patterns
                signature = '_'.join(sorted(analysis['functionality_patterns']))
                pattern_groups[signature].append(analysis['file_path'])
        
        # Group by filename patterns
        filename_groups = defaultdict(list)
        for analysis in all_files:
            filename = analysis['filename'].lower()
            # Remove common suffixes and variations
            clean_name = re.sub(r'[_\-\s]*\d+[_\-\s]*$', '', filename)
            clean_name = re.sub(r'[_\-\s]*\([^)]*\)[_\-\s]*$', '', clean_name)
            clean_name = re.sub(r'[_\-\s]*copy[_\-\s]*$', '', clean_name)
            clean_name = re.sub(r'[_\-\s]*variants[_\-\s]*$', '', clean_name)
            clean_name = clean_name.strip('._-')
            
            if clean_name:
                filename_groups[clean_name].append(analysis['file_path'])
        
        # Combine similar groups
        similar_groups = {}
        group_id = 0
        
        # Process pattern groups
        for pattern, files in pattern_groups.items():
            if len(files) > 1:
                similar_groups[f"pattern_{group_id}"] = files
                group_id += 1
        
        # Process filename groups
        for name, files in filename_groups.items():
            if len(files) > 1:
                similar_groups[f"filename_{group_id}"] = files
                group_id += 1
        
        logger.info(f"Found {len(similar_groups)} groups of similar files")
        return similar_groups

    def calculate_similarity(self, file1: Path, file2: Path) -> float:
        """Calculate similarity between two files."""
        try:
            analysis1 = self.content_analysis.get(file1)
            analysis2 = self.content_analysis.get(file2)
            
            if not analysis1 or not analysis2:
                return 0.0
            
            # Compare functionality patterns
            patterns1 = set(analysis1['functionality_patterns'])
            patterns2 = set(analysis2['functionality_patterns'])
            pattern_similarity = len(patterns1.intersection(patterns2)) / max(len(patterns1.union(patterns2)), 1)
            
            # Compare imports
            imports1 = set(analysis1['imports'])
            imports2 = set(analysis2['imports'])
            import_similarity = len(imports1.intersection(imports2)) / max(len(imports1.union(imports2)), 1)
            
            # Compare functions
            funcs1 = set(analysis1['functions'])
            funcs2 = set(analysis2['functions'])
            func_similarity = len(funcs1.intersection(funcs2)) / max(len(funcs1.union(funcs2)), 1)
            
            # Compare content using difflib
            content1 = analysis1['content']
            content2 = analysis2['content']
            content_similarity = difflib.SequenceMatcher(None, content1, content2).ratio()
            
            # Weighted average
            total_similarity = (
                pattern_similarity * 0.3 +
                import_similarity * 0.2 +
                func_similarity * 0.2 +
                content_similarity * 0.3
            )
            
            return total_similarity
            
        except Exception as e:
            logger.warning(f"Error calculating similarity between {file1} and {file2}: {e}")
            return 0.0

    def select_best_file(self, files: List[Path]) -> Path:
        """Select the best file from a group based on quality metrics."""
        if not files:
            return None
        
        if len(files) == 1:
            return files[0]
        
        # Analyze all files if not already done
        for file_path in files:
            if file_path not in self.content_analysis:
                self.content_analysis[file_path] = self.analyze_file_content(file_path)
        
        # Score files based on quality metrics
        scored_files = []
        for file_path in files:
            analysis = self.content_analysis[file_path]
            
            # Calculate composite score
            score = (
                analysis['quality_score'] * 10 +  # Quality indicators
                analysis['complexity_score'] * 2 +  # Complexity
                len(analysis['functions']) * 3 +  # Number of functions
                len(analysis['classes']) * 5 +  # Number of classes
                (1 if analysis['openai_usage'] else 0) * 5 +  # OpenAI usage
                (1 if analysis['quality_indicators'].get('has_docstring') else 0) * 3 +  # Docstrings
                (1 if analysis['quality_indicators'].get('has_error_handling') else 0) * 2 +  # Error handling
                analysis['size'] // CONSTANT_1000  # Size bonus
            )
            
            scored_files.append((file_path, score, analysis))
        
        # Sort by score (descending)
        scored_files.sort(key=lambda x: x[1], reverse=True)
        
        best_file = scored_files[0][0]
        logger.info(f"Selected best file: {best_file.name} (score: {scored_files[0][1]})")
        
        return best_file

    def create_merged_file(self, files: List[Path], output_path: Path) -> bool:
        """Create a merged file combining the best features from multiple files."""
        try:
            if not files:
                return False
            
            # Select the best base file
            base_file = self.select_best_file(files)
            if not base_file:
                return False
            
            # Read base file content
            with open(base_file, 'r', encoding='utf-8') as f:
                base_content = f.read()
            
            # Analyze all files to extract unique features
            all_analyses = []
            for file_path in files:
                if file_path not in self.content_analysis:
                    self.content_analysis[file_path] = self.analyze_file_content(file_path)
                all_analyses.append(self.content_analysis[file_path])
            
            # Extract unique functions, classes, and imports
            all_functions = set()
            all_classes = set()
            all_imports = set()
            all_docstrings = set()
            
            for analysis in all_analyses:
                all_functions.update(analysis['functions'])
                all_classes.update(analysis['classes'])
                all_imports.update(analysis['imports'])
                all_docstrings.update(analysis['docstrings'])
            
            # Create merged content
            merged_content = self._create_merged_content(
                base_content, all_functions, all_classes, all_imports, all_docstrings, files
            )
            
            # Write merged file
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(merged_content)
            
            logger.info(f"Created merged file: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"Error creating merged file {output_path}: {e}")
            return False

    def _create_merged_content(self, base_content: str, all_functions: Set[str], 
                             all_classes: Set[str], all_imports: Set[str], 
                             all_docstrings: Set[str], source_files: List[Path]) -> str:
        """Create merged content combining features from multiple files."""
        
        # Start with enhanced header
        merged_content = f'''#!/usr/bin/env python3
"""
Merged Content Analysis Tool

This file was automatically merged from the following source files:
{chr(10).join(f"- {f}" for f in source_files)}

Combines the best features and functionality from multiple similar files.
"""

'''
        
        # Add all unique imports
        if all_imports:
            merged_content += "# Imports from all source files\n"
            for imp in sorted(all_imports):
                merged_content += f"{imp}\n"
            merged_content += Path("\n")
        
        # Add docstrings
        if all_docstrings:
            merged_content += "# Documentation from source files\n"
            for doc in all_docstrings:
                if doc.strip():
                    merged_content += f"{doc}\n"
            merged_content += Path("\n")
        
        # Add the main content (simplified version of base content)
        # Remove the original header and basic structure
        lines = base_content.split('\n')
        content_start = 0
        
        # Find where the actual code starts (after imports and docstrings)
        for i, line in enumerate(lines):
            if line.strip() and not line.strip().startswith('#') and not line.strip().startswith('"""') and not line.strip().startswith("'''") and not line.strip().startswith('import ') and not line.strip().startswith('from '):
                content_start = i
                break
        
        # Add the main content
        main_content = '\n'.join(lines[content_start:])
        merged_content += main_content
        
        return merged_content

    def run_deep_analysis_and_merge(self):
        """Run the complete deep analysis and merge process."""
        logger.info("Starting deep analysis and merge process...")
        
        try:
            # Step 1: Find exact duplicates
            exact_duplicates = self.find_exact_duplicates()
            
            # Step 2: Find similar files
            similar_groups = self.find_similar_files()
            
            # Step 3: Create merged directory
            self.merged_dir.mkdir(exist_ok=True)
            
            # Step 4: Process exact duplicates
            logger.info("Processing exact duplicates...")
            for hash_val, files in exact_duplicates.items():
                best_file = self.select_best_file(files)
                if best_file:
                    # Copy best file to merged directory
                    target_path = self.merged_dir / best_file.name
                    self._copy_file(best_file, target_path)
                    logger.info(f"Kept best duplicate: {best_file.name}")
            
            # Step 5: Process similar files
            logger.info("Processing similar files...")
            for group_name, files in similar_groups.items():
                if len(files) > 1:
                    # Create merged file
                    merged_filename = self._generate_merged_filename(files)
                    merged_path = self.merged_dir / merged_filename
                    
                    if self.create_merged_file(files, merged_path):
                        logger.info(f"Created merged file: {merged_filename} from {len(files)} files")
                    else:
                        # Fallback: just copy the best file
                        best_file = self.select_best_file(files)
                        if best_file:
                            target_path = self.merged_dir / best_file.name
                            self._copy_file(best_file, target_path)
                            logger.info(f"Kept best file: {best_file.name}")
            
            # Step 6: Generate analysis report
            self._generate_analysis_report(exact_duplicates, similar_groups)
            
            logger.info("Deep analysis and merge process completed!")
            
        except Exception as e:
            logger.error(f"Error during deep analysis and merge: {e}")
            raise

    def _copy_file(self, src: Path, dst: Path):
        """Copy file from source to destination."""
        try:
            import shutil
            dst.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dst)
        except Exception as e:
            logger.error(f"Error copying {src} to {dst}: {e}")

    def _generate_merged_filename(self, files: List[Path]) -> str:
        """Generate a filename for merged files."""
        # Get common base name
        base_names = [f.stem for f in files]
        common_prefix = os.path.commonprefix(base_names)
        
        if common_prefix:
            return f"{common_prefix}_merged.py"
        else:
            return f"merged_{len(files)}_files.py"

    def _generate_analysis_report(self, exact_duplicates: Dict[str, List[Path]], 
                                similar_groups: Dict[str, List[Path]]):
        """Generate comprehensive analysis report."""
        report_path = self.merged_dir / "DEEP_ANALYSIS_REPORT.md"
        
        with open(report_path, 'w') as f:
            f.write("# Deep Analysis and Merge Report\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## Overview\n\n")
            f.write(f"- **Exact duplicate groups:** {len(exact_duplicates)}\n")
            f.write(f"- **Similar file groups:** {len(similar_groups)}\n")
            f.write(f"- **Total files analyzed:** {len(self.content_analysis)}\n\n")
            
            f.write("## Exact Duplicates Found\n\n")
            for hash_val, files in exact_duplicates.items():
                f.write(f"### Group {hash_val[:8]}...\n")
                f.write(f"Files: {len(files)}\n")
                for file_path in files:
                    f.write(f"- {file_path}\n")
                f.write(Path("\n"))
            
            f.write("## Similar File Groups\n\n")
            for group_name, files in similar_groups.items():
                f.write(f"### {group_name}\n")
                f.write(f"Files: {len(files)}\n")
                for file_path in files:
                    analysis = self.content_analysis.get(file_path, {})
                    f.write(f"- {file_path} (Quality: {analysis.get('quality_score', 0)})\n")
                f.write(Path("\n"))
            
            f.write("## Quality Analysis Summary\n\n")
            quality_stats = defaultdict(int)
            for analysis in self.content_analysis.values():
                quality_stats[analysis.get('quality_score', 0)] += 1
            
            for score, count in sorted(quality_stats.items()):
                f.write(f"- Quality Score {score}: {count} files\n")
            
            f.write("\n## Functionality Patterns\n\n")
            pattern_stats = defaultdict(int)
            for analysis in self.content_analysis.values():
                for pattern in analysis.get('functionality_patterns', []):
                    pattern_stats[pattern] += 1
            
            for pattern, count in sorted(pattern_stats.items(), key=lambda x: x[1], reverse=True):
                f.write(f"- {pattern}: {count} files\n")

# From intelligent_analyzer_merger.py
    def analyze_file_structure(self, file_path: Path) -> Dict[str, Any]:
        """Analyze the structure and content of a file."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            analysis = {
                'file_path': file_path,
                'filename': file_path.name,
                'extension': file_path.suffix,
                'size': file_path.stat().st_size,
                'content': content,
                'lines': content.split('\n'),
                'line_count': len(content.split('\n')),
                'functions': [],
                'classes': [],
                'imports': [],
                'docstrings': [],
                'comments': [],
                'error_handling': [],
                'logging': [],
                'main_function': False,
                'type_hints': False,
                'quality_score': 0
            }
            
            # Parse Python files
            if file_path.suffix == '.py':
                try:
                    tree = ast.parse(content)
                    analysis.update(self._analyze_python_ast(tree))
                except SyntaxError as e:
                    logger.warning(f"Syntax error in {file_path}: {e}")
            
            # Analyze shell scripts
            elif file_path.suffix == '.sh':
                analysis.update(self._analyze_shell_script(content))
            
            # Analyze markdown files
            elif file_path.suffix == '.md':
                analysis.update(self._analyze_markdown(content))
            
            # Calculate quality score
            analysis['quality_score'] = self._calculate_quality_score(analysis)
            
            return analysis
            
        except Exception as e:
            logger.warning(f"Error analyzing {file_path}: {e}")
            return {
                'file_path': file_path,
                'filename': file_path.name,
                'extension': file_path.suffix,
                'size': 0,
                'content': '',
                'lines': [],
                'line_count': 0,
                'functions': [],
                'classes': [],
                'imports': [],
                'docstrings': [],
                'comments': [],
                'error_handling': [],
                'logging': [],
                'main_function': False,
                'type_hints': False,
                'quality_score': 0
            }

# From intelligent_analyzer_merger.py
    def _analyze_python_ast(self, tree: ast.AST) -> Dict[str, Any]:
        """Analyze Python AST for functions, classes, imports, etc."""
        analysis = {
            'functions': [],
            'classes': [],
            'imports': [],
            'docstrings': [],
            'comments': [],
            'error_handling': [],
            'logging': [],
            'main_function': False,
            'type_hints': False
        }
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                func_info = {
                    'name': node.name,
                    'args': [arg.arg for arg in node.args.args],
                    'docstring': ast.get_docstring(node),
                    'line_start': node.lineno,
                    'line_end': node.end_lineno if hasattr(node, 'end_lineno') else node.lineno,
                    'is_async': isinstance(node, ast.AsyncFunctionDef),
                    'has_type_hints': any(arg.annotation for arg in node.args.args) or node.returns
                }
                analysis['functions'].append(func_info)
                
                if node.name == 'main':
                    analysis['main_function'] = True
                
                if func_info['has_type_hints']:
                    analysis['type_hints'] = True
                
                if func_info['docstring']:
                    analysis['docstrings'].append(func_info['docstring'])
            
            elif isinstance(node, ast.ClassDef):
                class_info = {
                    'name': node.name,
                    'bases': [base.id if isinstance(base, ast.Name) else str(base) for base in node.bases],
                    'docstring': ast.get_docstring(node),
                    'line_start': node.lineno,
                    'line_end': node.end_lineno if hasattr(node, 'end_lineno') else node.lineno,
                    'methods': []
                }
                
                for item in node.body:
                    if isinstance(item, ast.FunctionDef):
                        class_info['methods'].append(item.name)
                
                analysis['classes'].append(class_info)
                
                if class_info['docstring']:
                    analysis['docstrings'].append(class_info['docstring'])
            
            elif isinstance(node, (ast.Import, ast.ImportFrom)):
                import_info = {
                    'type': 'import' if isinstance(node, ast.Import) else 'from_import',
                    'module': node.module if isinstance(node, ast.ImportFrom) else None,
                    'names': [alias.name for alias in node.names],
                    'line': node.lineno
                }
                analysis['imports'].append(import_info)
            
            elif isinstance(node, ast.Expr) and isinstance(node.value, ast.Constant):
                if isinstance(node.value.value, str) and len(node.value.value) > 10:
                    analysis['docstrings'].append(node.value.value)
            
            elif isinstance(node, ast.Try):
                analysis['error_handling'].append({
                    'line': node.lineno,
                    'handlers': len(node.handlers)
                })
            
            elif isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    if node.func.id in ['logger', 'logging', 'print']:
                        analysis['logging'].append({
                            'function': node.func.id,
                            'line': node.lineno
                        })
        
        return analysis

# From intelligent_analyzer_merger.py
    def _analyze_shell_script(self, content: str) -> Dict[str, Any]:
        """Analyze shell script content."""
        lines = content.split('\n')
        
        analysis = {
            'functions': [],
            'classes': [],
            'imports': [],
            'docstrings': [],
            'comments': [],
            'error_handling': [],
            'logging': [],
            'main_function': False,
            'type_hints': False
        }
        
        # Find functions
        for i, line in enumerate(lines):
            if re.match(r'^\s*\w+\s*\(\s*\)\s*\{', line):
                func_name = re.match(r'^\s*(\w+)\s*\(', line).group(1)
                analysis['functions'].append({
                    'name': func_name,
                    'line_start': i + 1,
                    'line_end': i + 1
                })
        
        # Find comments
        for i, line in enumerate(lines):
            if line.strip().startswith('#'):
                analysis['comments'].append({
                    'line': i + 1,
                    'content': line.strip()
                })
        
        # Find error handling
        for i, line in enumerate(lines):
            if 'set -e' in line or 'trap' in line or 'if' in line and 'then' in line:
                analysis['error_handling'].append({
                    'line': i + 1,
                    'type': 'error_handling'
                })
        
        # Find logging
        for i, line in enumerate(lines):
            if 'echo' in line or 'printf' in line:
                analysis['logging'].append({
                    'line': i + 1,
                    'type': 'echo'
                })
        
        return analysis

# From intelligent_analyzer_merger.py
    def _analyze_markdown(self, content: str) -> Dict[str, Any]:
        """Analyze markdown content."""
        lines = content.split('\n')
        
        analysis = {
            'functions': [],
            'classes': [],
            'imports': [],
            'docstrings': [],
            'comments': [],
            'error_handling': [],
            'logging': [],
            'main_function': False,
            'type_hints': False
        }
        
        # Find code blocks
        in_code_block = False
        for i, line in enumerate(lines):
            if line.strip().startswith('```'):
                in_code_block = not in_code_block
            elif in_code_block and line.strip():
                analysis['comments'].append({
                    'line': i + 1,
                    'content': line.strip()
                })
        
        return analysis

# From intelligent_analyzer_merger.py
    def _calculate_quality_score(self, analysis: Dict[str, Any]) -> float:
        """Calculate quality score for a file."""
        score = 0
        
        # Base score from file size and complexity
        score += min(analysis['line_count'] * 0.1, 10)
        score += min(analysis['size'] / CONSTANT_1000, 5)
        
        # Function and class count
        score += len(analysis['functions']) * 2
        score += len(analysis['classes']) * 3
        
        # Documentation
        score += len(analysis['docstrings']) * 3
        score += len(analysis['comments']) * 0.5
        
        # Error handling
        score += len(analysis['error_handling']) * 2
        
        # Logging
        score += len(analysis['logging']) * 1
        
        # Type hints
        if analysis['type_hints']:
            score += 5
        
        # Main function
        if analysis['main_function']:
            score += 3
        
        # Import count (indicates dependencies)
        score += min(len(analysis['imports']) * 0.5, 5)
        
        return round(score, 2)

# From intelligent_analyzer_merger.py
    def analyze_all_files(self):
        """Analyze all files in the directory."""
        logger.info("Starting comprehensive file analysis...")
        
        file_count = 0
        for file_path in self.base_dir.rglob("*"):
            if file_path.is_file() and file_path.suffix in ['.py', '.sh', '.md', '.txt', '.json']:
                if not any(part.startswith('.') for part in file_path.parts):
                    analysis = self.analyze_file_structure(file_path)
                    self.file_analysis[file_path] = analysis
                    file_count += 1
                    
                    if file_count % 50 == 0:
                        logger.info(f"Analyzed {file_count} files...")
        
        logger.info(f"Analysis complete! Processed {file_count} files.")
        
        # Group files by functionality
        self._group_by_functionality()
        
        # Analyze dependencies
        self._analyze_dependencies()

# From intelligent_analyzer_merger.py
    def _group_by_functionality(self):
        """Group files by their functionality."""
        logger.info("Grouping files by functionality...")
        
        for file_path, analysis in self.file_analysis.items():
            functionality = self._determine_functionality(analysis)
            self.functionality_groups[functionality].append(file_path)
        
        logger.info(f"Grouped files into {len(self.functionality_groups)} functionality categories")

# From intelligent_analyzer_merger.py
    def _determine_functionality(self, analysis: Dict[str, Any]) -> str:
        """Determine the primary functionality of a file."""
        content = analysis['content'].lower()
        filename = analysis['filename'].lower()
        
        # Check for specific functionality patterns
        if any(keyword in content for keyword in ['analyze', 'analysis', 'analyzer']):
            return 'analysis'
        elif any(keyword in content for keyword in ['transcribe', 'transcript', 'whisper', 'speech']):
            return 'transcription'
        elif any(keyword in content for keyword in ['generate', 'create', 'build', 'html', 'csv']):
            return 'generation'
        elif any(keyword in content for keyword in ['process', 'convert', 'mp3', 'mp4', 'ffmpeg']):
            return 'processing'
        elif any(keyword in content for keyword in ['scrape', 'suno', 'beautifulsoup', 'requests']):
            return 'web_scraping'
        elif any(keyword in content for keyword in ['organize', 'sort', 'manage', 'file']):
            return 'organization'
        elif filename.endswith('.sh'):
            return 'scripts'
        elif filename.endswith('.md'):
            return 'documentation'
        else:
            return 'utilities'

# From intelligent_analyzer_merger.py
    def _analyze_dependencies(self):
        """Analyze dependencies between files."""
        logger.info("Analyzing file dependencies...")
        
        for file_path, analysis in self.file_analysis.items():
            if analysis['extension'] == '.py':
                dependencies = set()
                for import_info in analysis['imports']:
                    if import_info['type'] == 'import':
                        for name in import_info['names']:
                            dependencies.add(name)
                    elif import_info['type'] == 'from_import':
                        if import_info['module']:
                            dependencies.add(import_info['module'])
                
                self.dependency_graph[file_path] = dependencies

# From intelligent_analyzer_merger.py
    def compare_and_merge_functionality(self):
        """Compare files within each functionality group and merge them."""
        logger.info("Starting functionality comparison and merging...")
        
        self.analysis_dir.mkdir(exist_ok=True)
        self.unified_dir.mkdir(exist_ok=True)
        self.removed_dir.mkdir(exist_ok=True)
        
        for functionality, files in self.functionality_groups.items():
            if len(files) > 1:
                logger.info(f"Processing {functionality} group with {len(files)} files...")
                self._merge_functionality_group(functionality, files)
            else:
                # Single file, just move to unified directory
                self._move_to_unified(files[0], functionality)

# From intelligent_analyzer_merger.py
    def _merge_functionality_group(self, functionality: str, files: List[Path]):
        """Merge files within a functionality group."""
        # Analyze similarities and differences
        similarities = self._find_similarities(files)
        differences = self._find_differences(files)
        
        # Select the best base file
        best_file = self._select_best_base_file(files)
        
        # Create merged content
        merged_content = self._create_merged_content(best_file, files, similarities, differences)
        
        # Save merged file
        merged_path = self.unified_dir / f"unified_{functionality}.py"
        with open(merged_path, 'w', encoding='utf-8') as f:
            f.write(merged_content)
        
        logger.info(f"Created unified {functionality} file: {merged_path}")
        
        # Move original files to removed directory
        for file_path in files:
            self._move_to_removed(file_path)

# From intelligent_analyzer_merger.py
    def _find_similarities(self, files: List[Path]) -> Dict[str, List[Path]]:
        """Find similar functions, classes, and patterns across files."""
        similarities = defaultdict(list)
        
        for i, file1 in enumerate(files):
            for j, file2 in enumerate(files[i+1:], i+1):
                analysis1 = self.file_analysis[file1]
                analysis2 = self.file_analysis[file2]
                
                # Compare functions
                for func1 in analysis1['functions']:
                    for func2 in analysis2['functions']:
                        if func1['name'] == func2['name']:
                            similarities[f"function_{func1['name']}"].extend([file1, file2])
                
                # Compare classes
                for class1 in analysis1['classes']:
                    for class2 in analysis2['classes']:
                        if class1['name'] == class2['name']:
                            similarities[f"class_{class1['name']}"].extend([file1, file2])
        
        return similarities

# From intelligent_analyzer_merger.py
    def _find_differences(self, files: List[Path]) -> Dict[str, List[Path]]:
        """Find unique functions, classes, and patterns in files."""
        differences = defaultdict(list)
        
        all_functions = defaultdict(list)
        all_classes = defaultdict(list)
        
        for file_path in files:
            analysis = self.file_analysis[file_path]
            for func in analysis['functions']:
                all_functions[func['name']].append(file_path)
            for cls in analysis['classes']:
                all_classes[cls['name']].append(file_path)
        
        # Find unique functions
        for func_name, file_list in all_functions.items():
            if len(file_list) == 1:
                differences[f"unique_function_{func_name}"].extend(file_list)
        
        # Find unique classes
        for class_name, file_list in all_classes.items():
            if len(file_list) == 1:
                differences[f"unique_class_{class_name}"].extend(file_list)
        
        return differences

# From intelligent_analyzer_merger.py
    def _select_best_base_file(self, files: List[Path]) -> Path:
        """Select the best file to use as the base for merging."""
        best_file = None
        best_score = -1
        
        for file_path in files:
            analysis = self.file_analysis[file_path]
            score = analysis['quality_score']
            
            if score > best_score:
                best_score = score
                best_file = file_path
        
        logger.info(f"Selected best base file: {best_file.name} (score: {best_score})")
        return best_file

# From intelligent_analyzer_merger.py
    def _create_comprehensive_header(self, files: List[Path]) -> str:
        """Create a comprehensive header for the merged file."""
        header = f'''#!/usr/bin/env python3
"""
UNIFIED SOLUTION - Comprehensive File Manager

This file combines the best functionality from {len(files)} files:
{chr(10).join(f"- {f.name}" for f in files)}

Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Total files analyzed: {len(self.file_analysis)}
Total functionality groups: {len(self.functionality_groups)}

This unified solution provides:
- File processing and conversion
- Content analysis and generation
- Transcription and audio processing
- Web scraping and data extraction
- File organization and management
- Comprehensive error handling and logging
"""

'''
        return header

# From intelligent_analyzer_merger.py
    def _move_to_unified(self, file_path: Path, functionality: str):
        """Move a single file to the unified directory."""
        target_path = self.unified_dir / f"unified_{functionality}{file_path.suffix}"
        shutil.copy2(file_path, target_path)
        logger.info(f"Moved {file_path.name} to unified directory")

# From intelligent_analyzer_merger.py
    def _move_to_removed(self, file_path: Path):
        """Move a file to the removed directory."""
        target_path = self.removed_dir / file_path.name
        shutil.move(str(file_path), str(target_path))
        logger.info(f"Moved {file_path.name} to removed directory")

# From intelligent_analyzer_merger.py
    def generate_analysis_report(self):
        """Generate comprehensive analysis report."""
        report_path = self.analysis_dir / "COMPREHENSIVE_ANALYSIS_REPORT.md"
        
        with open(report_path, 'w') as f:
            f.write("# Comprehensive File Analysis Report\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## Overview\n\n")
            f.write(f"- **Total files analyzed:** {len(self.file_analysis)}\n")
            f.write(f"- **Functionality groups:** {len(self.functionality_groups)}\n")
            f.write(f"- **Unified directory:** {self.unified_dir}\n")
            f.write(f"- **Removed directory:** {self.removed_dir}\n\n")
            
            f.write("## Functionality Groups\n\n")
            for functionality, files in self.functionality_groups.items():
                f.write(f"### {functionality.title()}\n")
                f.write(f"Files: {len(files)}\n")
                for file_path in files:
                    analysis = self.file_analysis[file_path]
                    f.write(f"- {file_path.name} (Quality: {analysis['quality_score']:.2f})\n")
                f.write(Path("\n"))
            
            f.write("## Quality Analysis\n\n")
            quality_scores = [analysis['quality_score'] for analysis in self.file_analysis.values()]
            f.write(f"- **Average quality score:** {sum(quality_scores) / len(quality_scores):.2f}\n")
            f.write(f"- **Highest quality score:** {max(quality_scores):.2f}\n")
            f.write(f"- **Lowest quality score:** {min(quality_scores):.2f}\n\n")
        
        logger.info(f"Analysis report generated: {report_path}")

# From intelligent_analyzer_merger.py
    def run_intelligent_analysis(self):
        """Run the complete intelligent analysis and merging process."""
        logger.info("Starting intelligent analysis and merging process...")
        
        try:
            # Step 1: Analyze all files
            self.analyze_all_files()
            
            # Step 2: Compare and merge functionality
            self.compare_and_merge_functionality()
            
            # Step 3: Generate analysis report
            self.generate_analysis_report()
            
            logger.info("Intelligent analysis and merging process completed!")
            
            # Print summary
            logger.info(f"\n Intelligent Analysis and Merging Complete!")
            logger.info(f" Files analyzed: {len(self.file_analysis)}")
            logger.info(f" Functionality groups: {len(self.functionality_groups)}")
            logger.info(f" Unified solution: {self.unified_dir}")
            logger.info(f" Removed files: {self.removed_dir}")
            logger.info(f" Analysis report: {self.analysis_dir}/COMPREHENSIVE_ANALYSIS_REPORT.md")
            
        except Exception as e:
        """__init__ function."""

            logger.error(f"Error during intelligent analysis: {e}")
            raise

# From intelligent_analyzer_merger.py
class IntelligentAnalyzerMerger:
    """Analyzes all files and creates one comprehensive solution."""
    
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.analysis_dir = self.base_dir / "ANALYSIS_RESULTS"
        self.unified_dir = self.base_dir / "UNIFIED_SOLUTION"
        self.removed_dir = self.base_dir / "REMOVED_FILES"
        
        # Analysis data
        self.file_analysis = {}
        self.function_analysis = {}
        self.class_analysis = {}
        self.import_analysis = {}
        self.dependency_graph = {}
        self.functionality_groups = defaultdict(list)
        self.quality_scores = {}
        
        # Merged content
        self.unified_imports = set()
        self.unified_functions = {}
        self.unified_classes = {}
        self.unified_scripts = {}
        self.unified_docs = {}
        
    def analyze_file_structure(self, file_path: Path) -> Dict[str, Any]:
        """Analyze the structure and content of a file."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            analysis = {
                'file_path': file_path,
                'filename': file_path.name,
                'extension': file_path.suffix,
                'size': file_path.stat().st_size,
                'content': content,
                'lines': content.split('\n'),
                'line_count': len(content.split('\n')),
                'functions': [],
                'classes': [],
                'imports': [],
                'docstrings': [],
                'comments': [],
                'error_handling': [],
                'logging': [],
                'main_function': False,
                'type_hints': False,
                'quality_score': 0
            }
            
            # Parse Python files
            if file_path.suffix == '.py':
                try:
                    tree = ast.parse(content)
                    analysis.update(self._analyze_python_ast(tree))
                except SyntaxError as e:
                    logger.warning(f"Syntax error in {file_path}: {e}")
            
            # Analyze shell scripts
            elif file_path.suffix == '.sh':
                analysis.update(self._analyze_shell_script(content))
            
            # Analyze markdown files
            elif file_path.suffix == '.md':
                analysis.update(self._analyze_markdown(content))
            
            # Calculate quality score
            analysis['quality_score'] = self._calculate_quality_score(analysis)
            
            return analysis
            
        except Exception as e:
            logger.warning(f"Error analyzing {file_path}: {e}")
            return {
                'file_path': file_path,
                'filename': file_path.name,
                'extension': file_path.suffix,
                'size': 0,
                'content': '',
                'lines': [],
                'line_count': 0,
                'functions': [],
                'classes': [],
                'imports': [],
                'docstrings': [],
                'comments': [],
                'error_handling': [],
                'logging': [],
                'main_function': False,
                'type_hints': False,
                'quality_score': 0
            }
    
    def _analyze_python_ast(self, tree: ast.AST) -> Dict[str, Any]:
        """Analyze Python AST for functions, classes, imports, etc."""
        analysis = {
            'functions': [],
            'classes': [],
            'imports': [],
            'docstrings': [],
            'comments': [],
            'error_handling': [],
            'logging': [],
            'main_function': False,
            'type_hints': False
        }
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                func_info = {
                    'name': node.name,
                    'args': [arg.arg for arg in node.args.args],
                    'docstring': ast.get_docstring(node),
                    'line_start': node.lineno,
                    'line_end': node.end_lineno if hasattr(node, 'end_lineno') else node.lineno,
                    'is_async': isinstance(node, ast.AsyncFunctionDef),
                    'has_type_hints': any(arg.annotation for arg in node.args.args) or node.returns
                }
                analysis['functions'].append(func_info)
                
                if node.name == 'main':
                    analysis['main_function'] = True
                
                if func_info['has_type_hints']:
                    analysis['type_hints'] = True
                
                if func_info['docstring']:
                    analysis['docstrings'].append(func_info['docstring'])
            
            elif isinstance(node, ast.ClassDef):
                class_info = {
                    'name': node.name,
                    'bases': [base.id if isinstance(base, ast.Name) else str(base) for base in node.bases],
                    'docstring': ast.get_docstring(node),
                    'line_start': node.lineno,
                    'line_end': node.end_lineno if hasattr(node, 'end_lineno') else node.lineno,
                    'methods': []
                }
                
                for item in node.body:
                    if isinstance(item, ast.FunctionDef):
                        class_info['methods'].append(item.name)
                
                analysis['classes'].append(class_info)
                
                if class_info['docstring']:
                    analysis['docstrings'].append(class_info['docstring'])
            
            elif isinstance(node, (ast.Import, ast.ImportFrom)):
                import_info = {
                    'type': 'import' if isinstance(node, ast.Import) else 'from_import',
                    'module': node.module if isinstance(node, ast.ImportFrom) else None,
                    'names': [alias.name for alias in node.names],
                    'line': node.lineno
                }
                analysis['imports'].append(import_info)
            
            elif isinstance(node, ast.Expr) and isinstance(node.value, ast.Constant):
                if isinstance(node.value.value, str) and len(node.value.value) > 10:
                    analysis['docstrings'].append(node.value.value)
            
            elif isinstance(node, ast.Try):
                analysis['error_handling'].append({
                    'line': node.lineno,
                    'handlers': len(node.handlers)
                })
            
            elif isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    if node.func.id in ['logger', 'logging', 'print']:
                        analysis['logging'].append({
                            'function': node.func.id,
                            'line': node.lineno
                        })
        
        return analysis
    
    def _analyze_shell_script(self, content: str) -> Dict[str, Any]:
        """Analyze shell script content."""
        lines = content.split('\n')
        
        analysis = {
            'functions': [],
            'classes': [],
            'imports': [],
            'docstrings': [],
            'comments': [],
            'error_handling': [],
            'logging': [],
            'main_function': False,
            'type_hints': False
        }
        
        # Find functions
        for i, line in enumerate(lines):
            if re.match(r'^\s*\w+\s*\(\s*\)\s*\{', line):
                func_name = re.match(r'^\s*(\w+)\s*\(', line).group(1)
                analysis['functions'].append({
                    'name': func_name,
                    'line_start': i + 1,
                    'line_end': i + 1
                })
        
        # Find comments
        for i, line in enumerate(lines):
            if line.strip().startswith('#'):
                analysis['comments'].append({
                    'line': i + 1,
                    'content': line.strip()
                })
        
        # Find error handling
        for i, line in enumerate(lines):
            if 'set -e' in line or 'trap' in line or 'if' in line and 'then' in line:
                analysis['error_handling'].append({
                    'line': i + 1,
                    'type': 'error_handling'
                })
        
        # Find logging
        for i, line in enumerate(lines):
            if 'echo' in line or 'printf' in line:
                analysis['logging'].append({
                    'line': i + 1,
                    'type': 'echo'
                })
        
        return analysis
    
    def _analyze_markdown(self, content: str) -> Dict[str, Any]:
        """Analyze markdown content."""
        lines = content.split('\n')
        
        analysis = {
            'functions': [],
            'classes': [],
            'imports': [],
            'docstrings': [],
            'comments': [],
            'error_handling': [],
            'logging': [],
            'main_function': False,
            'type_hints': False
        }
        
        # Find code blocks
        in_code_block = False
        for i, line in enumerate(lines):
            if line.strip().startswith('```'):
                in_code_block = not in_code_block
            elif in_code_block and line.strip():
                analysis['comments'].append({
                    'line': i + 1,
                    'content': line.strip()
                })
        
        return analysis
    
    def _calculate_quality_score(self, analysis: Dict[str, Any]) -> float:
        """Calculate quality score for a file."""
        score = 0
        
        # Base score from file size and complexity
        score += min(analysis['line_count'] * 0.1, 10)
        score += min(analysis['size'] / CONSTANT_1000, 5)
        
        # Function and class count
        score += len(analysis['functions']) * 2
        score += len(analysis['classes']) * 3
        
        # Documentation
        score += len(analysis['docstrings']) * 3
        score += len(analysis['comments']) * 0.5
        
        # Error handling
        score += len(analysis['error_handling']) * 2
        
        # Logging
        score += len(analysis['logging']) * 1
        
        # Type hints
        if analysis['type_hints']:
            score += 5
        
        # Main function
        if analysis['main_function']:
            score += 3
        
        # Import count (indicates dependencies)
        score += min(len(analysis['imports']) * 0.5, 5)
        
        return round(score, 2)
    
    def analyze_all_files(self):
        """Analyze all files in the directory."""
        logger.info("Starting comprehensive file analysis...")
        
        file_count = 0
        for file_path in self.base_dir.rglob("*"):
            if file_path.is_file() and file_path.suffix in ['.py', '.sh', '.md', '.txt', '.json']:
                if not any(part.startswith('.') for part in file_path.parts):
                    analysis = self.analyze_file_structure(file_path)
                    self.file_analysis[file_path] = analysis
                    file_count += 1
                    
                    if file_count % 50 == 0:
                        logger.info(f"Analyzed {file_count} files...")
        
        logger.info(f"Analysis complete! Processed {file_count} files.")
        
        # Group files by functionality
        self._group_by_functionality()
        
        # Analyze dependencies
        self._analyze_dependencies()
    
    def _group_by_functionality(self):
        """Group files by their functionality."""
        logger.info("Grouping files by functionality...")
        
        for file_path, analysis in self.file_analysis.items():
            functionality = self._determine_functionality(analysis)
            self.functionality_groups[functionality].append(file_path)
        
        logger.info(f"Grouped files into {len(self.functionality_groups)} functionality categories")
    
    def _determine_functionality(self, analysis: Dict[str, Any]) -> str:
        """Determine the primary functionality of a file."""
        content = analysis['content'].lower()
        filename = analysis['filename'].lower()
        
        # Check for specific functionality patterns
        if any(keyword in content for keyword in ['analyze', 'analysis', 'analyzer']):
            return 'analysis'
        elif any(keyword in content for keyword in ['transcribe', 'transcript', 'whisper', 'speech']):
            return 'transcription'
        elif any(keyword in content for keyword in ['generate', 'create', 'build', 'html', 'csv']):
            return 'generation'
        elif any(keyword in content for keyword in ['process', 'convert', 'mp3', 'mp4', 'ffmpeg']):
            return 'processing'
        elif any(keyword in content for keyword in ['scrape', 'suno', 'beautifulsoup', 'requests']):
            return 'web_scraping'
        elif any(keyword in content for keyword in ['organize', 'sort', 'manage', 'file']):
            return 'organization'
        elif filename.endswith('.sh'):
            return 'scripts'
        elif filename.endswith('.md'):
            return 'documentation'
        else:
            return 'utilities'
    
    def _analyze_dependencies(self):
        """Analyze dependencies between files."""
        logger.info("Analyzing file dependencies...")
        
        for file_path, analysis in self.file_analysis.items():
            if analysis['extension'] == '.py':
                dependencies = set()
                for import_info in analysis['imports']:
                    if import_info['type'] == 'import':
                        for name in import_info['names']:
                            dependencies.add(name)
                    elif import_info['type'] == 'from_import':
                        if import_info['module']:
                            dependencies.add(import_info['module'])
                
                self.dependency_graph[file_path] = dependencies
    
    def compare_and_merge_functionality(self):
        """Compare files within each functionality group and merge them."""
        logger.info("Starting functionality comparison and merging...")
        
        self.analysis_dir.mkdir(exist_ok=True)
        self.unified_dir.mkdir(exist_ok=True)
        self.removed_dir.mkdir(exist_ok=True)
        
        for functionality, files in self.functionality_groups.items():
            if len(files) > 1:
                logger.info(f"Processing {functionality} group with {len(files)} files...")
                self._merge_functionality_group(functionality, files)
            else:
                # Single file, just move to unified directory
                self._move_to_unified(files[0], functionality)
    
    def _merge_functionality_group(self, functionality: str, files: List[Path]):
        """Merge files within a functionality group."""
        # Analyze similarities and differences
        similarities = self._find_similarities(files)
        differences = self._find_differences(files)
        
        # Select the best base file
        best_file = self._select_best_base_file(files)
        
        # Create merged content
        merged_content = self._create_merged_content(best_file, files, similarities, differences)
        
        # Save merged file
        merged_path = self.unified_dir / f"unified_{functionality}.py"
        with open(merged_path, 'w', encoding='utf-8') as f:
            f.write(merged_content)
        
        logger.info(f"Created unified {functionality} file: {merged_path}")
        
        # Move original files to removed directory
        for file_path in files:
            self._move_to_removed(file_path)
    
    def _find_similarities(self, files: List[Path]) -> Dict[str, List[Path]]:
        """Find similar functions, classes, and patterns across files."""
        similarities = defaultdict(list)
        
        for i, file1 in enumerate(files):
            for j, file2 in enumerate(files[i+1:], i+1):
                analysis1 = self.file_analysis[file1]
                analysis2 = self.file_analysis[file2]
                
                # Compare functions
                for func1 in analysis1['functions']:
                    for func2 in analysis2['functions']:
                        if func1['name'] == func2['name']:
                            similarities[f"function_{func1['name']}"].extend([file1, file2])
                
                # Compare classes
                for class1 in analysis1['classes']:
                    for class2 in analysis2['classes']:
                        if class1['name'] == class2['name']:
                            similarities[f"class_{class1['name']}"].extend([file1, file2])
        
        return similarities
    
    def _find_differences(self, files: List[Path]) -> Dict[str, List[Path]]:
        """Find unique functions, classes, and patterns in files."""
        differences = defaultdict(list)
        
        all_functions = defaultdict(list)
        all_classes = defaultdict(list)
        
        for file_path in files:
            analysis = self.file_analysis[file_path]
            for func in analysis['functions']:
                all_functions[func['name']].append(file_path)
            for cls in analysis['classes']:
                all_classes[cls['name']].append(file_path)
        
        # Find unique functions
        for func_name, file_list in all_functions.items():
            if len(file_list) == 1:
                differences[f"unique_function_{func_name}"].extend(file_list)
        
        # Find unique classes
        for class_name, file_list in all_classes.items():
            if len(file_list) == 1:
                differences[f"unique_class_{class_name}"].extend(file_list)
        
        return differences
    
    def _select_best_base_file(self, files: List[Path]) -> Path:
        """Select the best file to use as the base for merging."""
        best_file = None
        best_score = -1
        
        for file_path in files:
            analysis = self.file_analysis[file_path]
            score = analysis['quality_score']
            
            if score > best_score:
                best_score = score
                best_file = file_path
        
        logger.info(f"Selected best base file: {best_file.name} (score: {best_score})")
        return best_file
    
    def _create_merged_content(self, base_file: Path, files: List[Path], similarities: Dict, differences: Dict) -> str:
        """Create merged content combining all files."""
        base_analysis = self.file_analysis[base_file]
        
        # Start with base file content
        merged_lines = base_analysis['lines'].copy()
        
        # Add unique functions and classes from other files
        for file_path in files:
            if file_path != base_file:
                analysis = self.file_analysis[file_path]
                
                # Add unique functions
                for func in analysis['functions']:
                    if f"unique_function_{func['name']}" in differences:
                        func_lines = analysis['lines'][func['line_start']-1:func['line_end']]
                        merged_lines.extend(['', f"# From {file_path.name}"] + func_lines)
                
                # Add unique classes
                for cls in analysis['classes']:
                    if f"unique_class_{cls['name']}" in differences:
                        cls_lines = analysis['lines'][cls['line_start']-1:cls['line_end']]
                        merged_lines.extend(['', f"# From {file_path.name}"] + cls_lines)
        
        # Add comprehensive header
        header = self._create_comprehensive_header(files)
        
        return header + '\n'.join(merged_lines)
    
    def _create_comprehensive_header(self, files: List[Path]) -> str:
        """Create a comprehensive header for the merged file."""
        header = f'''#!/usr/bin/env python3
"""
UNIFIED SOLUTION - Comprehensive File Manager

This file combines the best functionality from {len(files)} files:
{chr(10).join(f"- {f.name}" for f in files)}

Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Total files analyzed: {len(self.file_analysis)}
Total functionality groups: {len(self.functionality_groups)}

This unified solution provides:
- File processing and conversion
- Content analysis and generation
- Transcription and audio processing
- Web scraping and data extraction
- File organization and management
- Comprehensive error handling and logging
"""

'''
        return header
    
    def _move_to_unified(self, file_path: Path, functionality: str):
        """Move a single file to the unified directory."""
        target_path = self.unified_dir / f"unified_{functionality}{file_path.suffix}"
        shutil.copy2(file_path, target_path)
        logger.info(f"Moved {file_path.name} to unified directory")
    
    def _move_to_removed(self, file_path: Path):
        """Move a file to the removed directory."""
        target_path = self.removed_dir / file_path.name
        shutil.move(str(file_path), str(target_path))
        logger.info(f"Moved {file_path.name} to removed directory")
    
    def generate_analysis_report(self):
        """Generate comprehensive analysis report."""
        report_path = self.analysis_dir / "COMPREHENSIVE_ANALYSIS_REPORT.md"
        
        with open(report_path, 'w') as f:
            f.write("# Comprehensive File Analysis Report\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## Overview\n\n")
            f.write(f"- **Total files analyzed:** {len(self.file_analysis)}\n")
            f.write(f"- **Functionality groups:** {len(self.functionality_groups)}\n")
            f.write(f"- **Unified directory:** {self.unified_dir}\n")
            f.write(f"- **Removed directory:** {self.removed_dir}\n\n")
            
            f.write("## Functionality Groups\n\n")
            for functionality, files in self.functionality_groups.items():
                f.write(f"### {functionality.title()}\n")
                f.write(f"Files: {len(files)}\n")
                for file_path in files:
                    analysis = self.file_analysis[file_path]
                    f.write(f"- {file_path.name} (Quality: {analysis['quality_score']:.2f})\n")
                f.write(Path("\n"))
            
            f.write("## Quality Analysis\n\n")
            quality_scores = [analysis['quality_score'] for analysis in self.file_analysis.values()]
            f.write(f"- **Average quality score:** {sum(quality_scores) / len(quality_scores):.2f}\n")
            f.write(f"- **Highest quality score:** {max(quality_scores):.2f}\n")
            f.write(f"- **Lowest quality score:** {min(quality_scores):.2f}\n\n")
        
        logger.info(f"Analysis report generated: {report_path}")
    
    def run_intelligent_analysis(self):
        """Run the complete intelligent analysis and merging process."""
        logger.info("Starting intelligent analysis and merging process...")
        
        try:
            # Step 1: Analyze all files
            self.analyze_all_files()
            
            # Step 2: Compare and merge functionality
            self.compare_and_merge_functionality()
            
            # Step 3: Generate analysis report
            self.generate_analysis_report()
            
            logger.info("Intelligent analysis and merging process completed!")
            
            # Print summary
            logger.info(f"\n Intelligent Analysis and Merging Complete!")
            logger.info(f" Files analyzed: {len(self.file_analysis)}")
            logger.info(f" Functionality groups: {len(self.functionality_groups)}")
            logger.info(f" Unified solution: {self.unified_dir}")
            logger.info(f" Removed files: {self.removed_dir}")
            logger.info(f" Analysis report: {self.analysis_dir}/COMPREHENSIVE_ANALYSIS_REPORT.md")
            
        except Exception as e:
            logger.error(f"Error during intelligent analysis: {e}")
            raise

# From ULTIMATE_FILE_MANAGER.py
    def upscale_image(self, image_path: Path, scale_factor: int = 2) -> Optional[Path]:
        """Upscale image using ffmpeg."""
        output_path = image_path.parent / f"{image_path.stem}_upscaled{image_path.suffix}"
        
        try:
            subprocess.run([
                "ffmpeg", "-i", str(image_path), "-vf", f"scale=iw*{scale_factor}:ih*{scale_factor}",
                str(output_path)
            ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            logger.info(f"Upscaled {image_path.name} by {scale_factor}x")
            return output_path
        except Exception as e:
            logger.error(f"Error upscaling {image_path}: {e}")
            return None

# From ULTIMATE_FILE_MANAGER.py
    def extract_metadata(self, file_path: Path) -> Dict:
        """Extract metadata from media file using ffprobe."""
        try:
            result = subprocess.run([
                "ffprobe", "-v", "quiet", "-print_format", "json", "-show_format", "-show_streams",
                str(file_path)
            ], capture_output=True, text=True, check=True)
            
            metadata = json.loads(result.stdout)
            
            # Extract useful information
            format_info = metadata.get('format', {})
            streams = metadata.get('streams', [])
            
            return {
                'filename': file_path.name,
                'duration': format_info.get('duration', '0'),
                'size': format_info.get('size', '0'),
                'bitrate': format_info.get('bit_rate', '0'),
                'format_name': format_info.get('format_name', 'unknown'),
                'streams': len(streams)
            }
        except Exception as e:
            logger.error(f"Error extracting metadata from {file_path}: {e}")
            return {}

# From ULTIMATE_FILE_MANAGER.py
    def batch_convert(self, input_dir: Path, output_dir: Optional[Path] = None, 
                     conversion_type: str = "mp4_to_mp3") -> Dict[Path, Optional[Path]]:
        """Batch convert files in a directory."""
        if not output_dir:
            output_dir = input_dir / "converted"
        output_dir.mkdir(exist_ok=True)
        
        results = {}
        files_to_process = []
        
        # Find files to process
        if conversion_type == "mp4_to_mp3":
            files_to_process = list(input_dir.rglob("*.mp4"))
        elif conversion_type == "mp3_to_mp4":
            files_to_process = list(input_dir.rglob("*.mp3"))
        
        for file_path in files_to_process:
            try:
                if conversion_type == "mp4_to_mp3":
                    result = self.convert_mp4_to_mp3(file_path)
                elif conversion_type == "mp3_to_mp4":
                    result = self.convert_mp3_to_mp4(file_path)
                else:
                    result = None
                
                results[file_path] = result
                
            except Exception as e:
                logger.error(f"Error processing {file_path}: {e}")
                results[file_path] = None
        
        logger.info(f"Processed {len(files_to_process)} files")
        return results

# From ULTIMATE_FILE_MANAGER.py
    def organize_by_file_type(self, source_dir: Path, target_dir: Optional[Path] = None) -> Dict[str, int]:
        """Organize files by their type/extension."""
        if not target_dir:
            target_dir = source_dir / "organized_by_type"
        
        target_dir.mkdir(exist_ok=True)
        
        # Create category directories
        for category in self.file_type_categories.keys():
            (target_dir / category).mkdir(exist_ok=True)
        
        # Create "other" directory for unrecognized types
        (target_dir / "other").mkdir(exist_ok=True)
        
        file_counts = defaultdict(int)
        
        for file_path in source_dir.rglob("*"):
            if file_path.is_file():
                extension = file_path.suffix.lower()
                category = self._get_file_category(extension)
                
                target_category_dir = target_dir / category
                target_file_path = target_category_dir / file_path.name
                
                # Handle duplicate names
                counter = 1
                original_target = target_file_path
                while target_file_path.exists():
                    stem = original_target.stem
                    suffix = original_target.suffix
                    target_file_path = target_category_dir / f"{stem}_{counter}{suffix}"
                    counter += 1
                
                try:
                    shutil.move(str(file_path), str(target_file_path))
                    file_counts[category] += 1
                    logger.info(f"Moved {file_path.name} to {category}/")
                except Exception as e:
                    logger.error(f"Error moving {file_path}: {e}")
        
        logger.info(f"Organized {sum(file_counts.values())} files by type")
        return dict(file_counts)

# From ULTIMATE_FILE_MANAGER.py
    def organize_by_date(self, source_dir: Path, target_dir: Optional[Path] = None, 
                        date_format: str = "year_month") -> Dict[str, int]:
        """Organize files by their creation or modification date."""
        if not target_dir:
            target_dir = source_dir / "organized_by_date"
        
        target_dir.mkdir(exist_ok=True)
        
        file_counts = defaultdict(int)
        
        for file_path in source_dir.rglob("*"):
            if file_path.is_file():
                try:
                    stat = file_path.stat()
                    mtime = stat.st_mtime
                    
                    if date_format == "year_month":
                        date_str = time.strftime("%Y-%m", time.localtime(mtime))
                    elif date_format == "year":
                        date_str = time.strftime("%Y", time.localtime(mtime))
                    elif date_format == "year_month_day":
                        date_str = time.strftime("%Y-%m-%d", time.localtime(mtime))
                    else:
                        date_str = "unknown"
                    
                    date_dir = target_dir / date_str
                    date_dir.mkdir(exist_ok=True)
                    
                    target_file_path = date_dir / file_path.name
                    self._handle_duplicate_file(file_path, target_file_path)
                    file_counts[date_str] += 1
                    
                except Exception as e:
                    logger.error(f"Error organizing {file_path} by date: {e}")
        
        logger.info(f"Organized {sum(file_counts.values())} files by date")
        return dict(file_counts)

# From ULTIMATE_FILE_MANAGER.py
    def organize_by_size(self, source_dir: Path, target_dir: Optional[Path] = None) -> Dict[str, int]:
        """Organize files by their size ranges."""
        if not target_dir:
            target_dir = source_dir / "organized_by_size"
        
        target_dir.mkdir(exist_ok=True)
        
        size_categories = {
            'tiny': (0, CONSTANT_1024),  # < 1KB
            'small': (CONSTANT_1024, CONSTANT_1024*CONSTANT_1024),  # 1KB - 1MB
            'medium': (CONSTANT_1024*CONSTANT_1024, 10*CONSTANT_1024*CONSTANT_1024),  # 1MB - 10MB
            'large': (10*CONSTANT_1024*CONSTANT_1024, CONSTANT_100*CONSTANT_1024*CONSTANT_1024),  # 10MB - 100MB
            'huge': (CONSTANT_100*CONSTANT_1024*CONSTANT_1024, float('inf'))  # > 100MB
        }
        
        # Create size category directories
        for category in size_categories.keys():
            (target_dir / category).mkdir(exist_ok=True)
        
        file_counts = defaultdict(int)
        
        for file_path in source_dir.rglob("*"):
            if file_path.is_file():
                try:
                    size = file_path.stat().st_size
                    category = self._get_size_category(size, size_categories)
                    
                    category_dir = target_dir / category
                    target_file_path = category_dir / file_path.name
                    self._handle_duplicate_file(file_path, target_file_path)
                    file_counts[category] += 1
                    
                except Exception as e:
                    logger.error(f"Error organizing {file_path} by size: {e}")
        
        logger.info(f"Organized {sum(file_counts.values())} files by size")
        return dict(file_counts)

# From ULTIMATE_FILE_MANAGER.py
    def clean_duplicates(self, duplicates: Dict[str, List[Path]], 
                        keep_strategy: str = "largest", archive_dir: Optional[Path] = None) -> int:
        """Clean duplicate files using specified strategy."""
        if not archive_dir:
            archive_dir = Path("DUPLICATES_ARCHIVE")
        archive_dir.mkdir(exist_ok=True)
        
        removed_count = 0
        
        for file_hash, files in duplicates.items():
            if len(files) > 1:
                if keep_strategy == "largest":
                    files.sort(key=lambda x: x.stat().st_size, reverse=True)
                elif keep_strategy == "newest":
                    files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                elif keep_strategy == "shortest_path":
                    files.sort(key=lambda x: len(x.parts))
                elif keep_strategy == "highest_quality":
                    file_analyses = [self.analyze_file_content(f) for f in files]
                    file_analyses.sort(key=lambda x: x['quality_score'], reverse=True)
                    files = [f['file_path'] for f in file_analyses]
                
                keep_file = files[0]
                duplicates_to_remove = files[1:]
                
                logger.info(f"Keeping: {keep_file}")
                logger.info(f"Archiving {len(duplicates_to_remove)} duplicates")
                
                for duplicate in duplicates_to_remove:
                    try:
                        archive_path = archive_dir / duplicate.name
                        # Handle duplicate names in archive
                        counter = 1
                        original_archive = archive_path
                        while archive_path.exists():
                            stem = original_archive.stem
                            suffix = original_archive.suffix
                            archive_path = original_archive.parent / f"{stem}_{counter}{suffix}"
                            counter += 1
                        
                        shutil.move(str(duplicate), str(archive_path))
                        removed_count += 1
                        logger.info(f"Archived: {duplicate}")
                    except Exception as e:
                        logger.error(f"Error archiving {duplicate}: {e}")
        
        logger.info(f"Archived {removed_count} duplicate files")
        return removed_count

# From ULTIMATE_FILE_MANAGER.py
    def comprehensive_cleanup(self, source_dir: Path, target_dir: Optional[Path] = None) -> Dict:
        """Perform comprehensive cleanup: deduplicate, organize, and process files."""
        if not target_dir:
            target_dir = source_dir / "CLEANED_ORGANIZED"
        
        target_dir.mkdir(exist_ok=True)
        
        logger.info("Starting comprehensive cleanup...")
        
        # Step 1: Find and clean duplicates
        duplicates = self.find_exact_duplicates(source_dir)
        similar_files = self.find_similar_files(source_dir)
        
        # Clean duplicates
        archived_duplicates = self.clean_duplicates(duplicates, "highest_quality", target_dir / "DUPLICATES_ARCHIVE")
        
        # Step 2: Organize by file type
        type_organization = self.organize_by_file_type(source_dir, target_dir / "by_type")
        
        # Step 3: Organize music files specifically
        music_files = list(source_dir.rglob("*.mp3"))
        if music_files:
            music_organization = self.organize_music_library(source_dir, "artist", target_dir / "music_by_artist")
        else:
            music_organization = {}
        
        # Step 4: Generate comprehensive report
        report = {
            'timestamp': datetime.now().isoformat(),
            'source_directory': str(source_dir),
            'target_directory': str(target_dir),
            'duplicates_found': len(duplicates),
            'duplicates_archived': archived_duplicates,
            'similar_groups': len(similar_files),
            'type_organization': type_organization,
            'music_organization': music_organization,
            'total_files_processed': sum(type_organization.values()) + sum(music_organization.values())
        }
        
        # Save report
        report_path = target_dir / "CLEANUP_REPORT.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info("Comprehensive cleanup completed!")
        return report

# From ULTIMATE_FILE_MANAGER.py
    def _get_file_category(self, extension: str) -> str:
        """Get category for file extension."""
        for category, extensions in self.file_type_categories.items():
            if extension in extensions:
                return category
        return "other"

# From ULTIMATE_FILE_MANAGER.py
    def _get_size_category(self, size: int, size_categories: Dict) -> str:
        """Get size category for file size."""
        for category, (min_size, max_size) in size_categories.items():
            if min_size <= size < max_size:
                return category
        return "huge"

# From ULTIMATE_FILE_MANAGER.py
    def _extract_artist_from_filename(self, filename: str) -> str:
        """Extract artist name from filename."""
        # Try different patterns
        for pattern_name, pattern in self.music_organize_patterns.items():
            match = re.match(pattern, filename)
            if match:
                return match.group(1).strip()
        
        # Fallback: use first part before any separator
        separators = [' - ', ' _ ', ' -', '_']
        for sep in separators:
            if sep in filename:
                return filename.split(sep)[0].strip()
        
        return "Unknown Artist"

# From ULTIMATE_FILE_MANAGER.py
    def _extract_artist_album_from_filename(self, filename: str) -> Tuple[str, str]:
        """Extract artist and album from filename."""
        # Try artist - album - title pattern
        match = re.match(self.music_organize_patterns['artist_album'], filename)
        if match:
            return match.group(1).strip(), match.group(2).strip()
        
        # Fallback
        return self._extract_artist_from_filename(filename), "Unknown Album"

# From ULTIMATE_FILE_MANAGER.py
    def _sanitize_filename(self, filename: str) -> str:
        """Sanitize filename for filesystem use."""
        # Remove invalid characters
        invalid_chars = '<>:"/\\|?*'
        for char in invalid_chars:
            filename = filename.replace(char, '_')
        
        # Remove extra spaces and dots
        filename = re.sub(r'\s+', ' ', filename).strip()
        filename = filename.strip('.')
        
        return filename

# From ULTIMATE_FILE_MANAGER.py
    def _handle_duplicate_file(self, source: Path, target: Path):
        """Handle duplicate file names when moving."""
        counter = 1
        original_target = target
        while target.exists():
            stem = original_target.stem
            suffix = original_target.suffix
            target = original_target.parent / f"{stem}_{counter}{suffix}"
        """__init__ function."""

            counter += 1
        
        shutil.move(str(source), str(target))

# From ULTIMATE_FILE_MANAGER.py
class UltimateFileManager:
    """Comprehensive file management tool combining processing, organization, and deduplication."""
    
    def __init__(self):
        self.client = client
        
        # File type categories for organization
        self.file_type_categories = {
            'audio': ['.mp3', '.wav', '.flac', '.m4a', '.aac', '.ogg'],
            'video': ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv'],
            'image': ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.svg'],
            'document': ['.pdf', '.doc', '.docx', '.txt', '.rtf', '.odt'],
            'archive': ['.zip', '.rar', '.7z', '.tar', '.gz'],
            'code': ['.py', '.js', '.html', '.css', '.java', '.cpp', '.c'],
            'data': ['.csv', '.json', '.xml', '.yaml', '.yml']
        }
        
        # Music organization patterns
        self.music_organize_patterns = {
            'artist_album': r'^(.+?)\s*-\s*(.+?)\s*-\s*(.+?)$',
            'artist_title': r'^(.+?)\s*-\s*(.+?)$',
            'album_track': r'^(\d+)\s*-\s*(.+?)$',
            'date_title': r'^(\d{4}-\d{2}-\d{2})\s*-\s*(.+?)$'
        }
        
        # Content similarity patterns for deduplication
        self.similarity_patterns = {
            'analyze_variants': [
                r'analyze.*\.py$', r'analyzer.*\.py$', r'.*analysis.*\.py$'
            ],
            'transcribe_variants': [
                r'trans.*\.py$', r'transcript.*\.py$', r'.*transcribe.*\.py$'
            ],
            'generate_variants': [
                r'generate.*\.py$', r'gen.*\.py$', r'.*create.*\.py$'
            ],
            'process_variants': [
                r'process.*\.py$', r'mp3.*\.py$', r'mp4.*\.py$', r'convert.*\.py$'
            ],
            'suno_variants': [
                r'suno.*\.py$', r'.*scrape.*\.py$', r'.*extract.*\.py$'
            ]
        }
        
        # File categories for organization
        self.categories = {
            'core_analysis': 'Core Analysis Scripts',
            'transcription': 'Transcription & Speech Processing',
            'generation': 'Content Generation',
            'processing': 'File Processing & Conversion',
            'web_scraping': 'Web Scraping & Data Extraction',
            'organization': 'File Organization & Management',
            'utilities': 'Utility Scripts',
            'experimental': 'Experimental & Test Scripts',
            'archived': 'Archived Scripts'
        }
        
        # File hash cache for duplicate detection
        self.file_hashes = {}
        self.duplicate_groups = defaultdict(list)
        self.processed_files = set()
    
    # ============================================================================
    # FILE PROCESSING METHODS (from Advanced File Processor)
    # ============================================================================
    
    def convert_mp4_to_mp3(self, mp4_path: Path, quality: str = "0") -> Optional[Path]:
        """Convert MP4 to MP3 using ffmpeg."""
        mp3_path = mp4_path.with_suffix('.mp3')
        if mp3_path.exists():
            logger.info(f"MP3 already exists: {mp3_path}")
            return mp3_path
        
        try:
            subprocess.run([
                "ffmpeg", "-i", str(mp4_path), "-q:a", quality, "-map", "a", str(mp3_path)
            ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            logger.info(f"Converted {mp4_path.name} to MP3")
            return mp3_path
        except Exception as e:
            logger.error(f"Error converting {mp4_path}: {e}")
            return None
    
    def convert_mp3_to_mp4(self, mp3_path: Path, image_path: Optional[Path] = None) -> Optional[Path]:
        """Convert MP3 to MP4 with optional cover image."""
        mp4_path = mp3_path.with_suffix('.mp4')
        if mp4_path.exists():
            logger.info(f"MP4 already exists: {mp4_path}")
            return mp4_path
        
        try:
            if image_path and image_path.exists():
                # Convert with cover image
                subprocess.run([
                    "ffmpeg", "-loop", "1", "-i", str(image_path), "-i", str(mp3_path),
                    "-c:v", "libx264", "-tune", "stillimage", "-c:a", "aac",
                    "-b:a", "192k", "-pix_fmt", "yuv420p", "-shortest", str(mp4_path)
                ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            else:
                # Convert without image (black screen)
                subprocess.run([
                    "ffmpeg", "-f", "lavfi", "-i", "color=c=black:s=1280x720:r=1",
                    "-i", str(mp3_path), "-c:v", "libx264", "-c:a", "aac",
                    "-b:a", "192k", "-shortest", str(mp4_path)
                ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            logger.info(f"Converted {mp3_path.name} to MP4")
            return mp4_path
        except Exception as e:
            logger.error(f"Error converting {mp3_path}: {e}")
            return None
    
    def split_audio(self, file_path: Path, segment_length: int = CONSTANT_300) -> List[Path]:
        """Split audio into smaller segments."""
        output_dir = file_path.parent / "segments"
        output_dir.mkdir(exist_ok=True)
        
        file_name_no_ext = file_path.stem
        command = [
            "ffmpeg", "-i", str(file_path), "-f", "segment",
            "-segment_time", str(segment_length), "-c", "copy",
            str(output_dir / f"{file_name_no_ext}_%03d.mp3")
        ]
        
        try:
            subprocess.run(command, check=True)
            segments = sorted(list(output_dir.glob("*.mp3")))
            logger.info(f"Split {file_path.name} into {len(segments)} segments")
            return segments
        except Exception as e:
            logger.error(f"Error splitting {file_path}: {e}")
            return []
    
    def upscale_image(self, image_path: Path, scale_factor: int = 2) -> Optional[Path]:
        """Upscale image using ffmpeg."""
        output_path = image_path.parent / f"{image_path.stem}_upscaled{image_path.suffix}"
        
        try:
            subprocess.run([
                "ffmpeg", "-i", str(image_path), "-vf", f"scale=iw*{scale_factor}:ih*{scale_factor}",
                str(output_path)
            ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            logger.info(f"Upscaled {image_path.name} by {scale_factor}x")
            return output_path
        except Exception as e:
            logger.error(f"Error upscaling {image_path}: {e}")
            return None
    
    def extract_metadata(self, file_path: Path) -> Dict:
        """Extract metadata from media file using ffprobe."""
        try:
            result = subprocess.run([
                "ffprobe", "-v", "quiet", "-print_format", "json", "-show_format", "-show_streams",
                str(file_path)
            ], capture_output=True, text=True, check=True)
            
            metadata = json.loads(result.stdout)
            
            # Extract useful information
            format_info = metadata.get('format', {})
            streams = metadata.get('streams', [])
            
            return {
                'filename': file_path.name,
                'duration': format_info.get('duration', '0'),
                'size': format_info.get('size', '0'),
                'bitrate': format_info.get('bit_rate', '0'),
                'format_name': format_info.get('format_name', 'unknown'),
                'streams': len(streams)
            }
        except Exception as e:
            logger.error(f"Error extracting metadata from {file_path}: {e}")
            return {}
    
    def batch_convert(self, input_dir: Path, output_dir: Optional[Path] = None, 
                     conversion_type: str = "mp4_to_mp3") -> Dict[Path, Optional[Path]]:
        """Batch convert files in a directory."""
        if not output_dir:
            output_dir = input_dir / "converted"
        output_dir.mkdir(exist_ok=True)
        
        results = {}
        files_to_process = []
        
        # Find files to process
        if conversion_type == "mp4_to_mp3":
            files_to_process = list(input_dir.rglob("*.mp4"))
        elif conversion_type == "mp3_to_mp4":
            files_to_process = list(input_dir.rglob("*.mp3"))
        
        for file_path in files_to_process:
            try:
                if conversion_type == "mp4_to_mp3":
                    result = self.convert_mp4_to_mp3(file_path)
                elif conversion_type == "mp3_to_mp4":
                    result = self.convert_mp3_to_mp4(file_path)
                else:
                    result = None
                
                results[file_path] = result
                
            except Exception as e:
                logger.error(f"Error processing {file_path}: {e}")
                results[file_path] = None
        
        logger.info(f"Processed {len(files_to_process)} files")
        return results
    
    # ============================================================================
    # FILE ORGANIZATION METHODS (from Advanced Organizer)
    # ============================================================================
    
    def organize_by_file_type(self, source_dir: Path, target_dir: Optional[Path] = None) -> Dict[str, int]:
        """Organize files by their type/extension."""
        if not target_dir:
            target_dir = source_dir / "organized_by_type"
        
        target_dir.mkdir(exist_ok=True)
        
        # Create category directories
        for category in self.file_type_categories.keys():
            (target_dir / category).mkdir(exist_ok=True)
        
        # Create "other" directory for unrecognized types
        (target_dir / "other").mkdir(exist_ok=True)
        
        file_counts = defaultdict(int)
        
        for file_path in source_dir.rglob("*"):
            if file_path.is_file():
                extension = file_path.suffix.lower()
                category = self._get_file_category(extension)
                
                target_category_dir = target_dir / category
                target_file_path = target_category_dir / file_path.name
                
                # Handle duplicate names
                counter = 1
                original_target = target_file_path
                while target_file_path.exists():
                    stem = original_target.stem
                    suffix = original_target.suffix
                    target_file_path = target_category_dir / f"{stem}_{counter}{suffix}"
                    counter += 1
                
                try:
                    shutil.move(str(file_path), str(target_file_path))
                    file_counts[category] += 1
                    logger.info(f"Moved {file_path.name} to {category}/")
                except Exception as e:
                    logger.error(f"Error moving {file_path}: {e}")
        
        logger.info(f"Organized {sum(file_counts.values())} files by type")
        return dict(file_counts)
    
    def organize_music_library(self, music_dir: Path, organize_by: str = "artist", 
                              target_dir: Optional[Path] = None) -> Dict[str, int]:
        """Organize music library by artist, album, or genre."""
        if not target_dir:
            target_dir = music_dir / "organized_music"
        
        target_dir.mkdir(exist_ok=True)
        
        file_counts = defaultdict(int)
        
        for file_path in music_dir.rglob("*.mp3"):
            try:
                if organize_by == "artist":
                    artist = self._extract_artist_from_filename(file_path.name)
                    artist_dir = target_dir / self._sanitize_filename(artist)
                    artist_dir.mkdir(exist_ok=True)
                    
                    target_file_path = artist_dir / file_path.name
                    self._handle_duplicate_file(file_path, target_file_path)
                    file_counts[artist] += 1
                
                elif organize_by == "album":
                    artist, album = self._extract_artist_album_from_filename(file_path.name)
                    album_dir = target_dir / self._sanitize_filename(artist) / self._sanitize_filename(album)
                    album_dir.mkdir(parents=True, exist_ok=True)
                    
                    target_file_path = album_dir / file_path.name
                    self._handle_duplicate_file(file_path, target_file_path)
                    file_counts[f"{artist}/{album}"] += 1
                
                elif organize_by == "genre":
                    # This would require metadata extraction
                    logger.warning("Genre organization requires metadata extraction - not implemented")
                    continue
                
            except Exception as e:
                logger.error(f"Error organizing {file_path}: {e}")
        
        logger.info(f"Organized {sum(file_counts.values())} music files by {organize_by}")
        return dict(file_counts)
    
    def organize_by_date(self, source_dir: Path, target_dir: Optional[Path] = None, 
                        date_format: str = "year_month") -> Dict[str, int]:
        """Organize files by their creation or modification date."""
        if not target_dir:
            target_dir = source_dir / "organized_by_date"
        
        target_dir.mkdir(exist_ok=True)
        
        file_counts = defaultdict(int)
        
        for file_path in source_dir.rglob("*"):
            if file_path.is_file():
                try:
                    stat = file_path.stat()
                    mtime = stat.st_mtime
                    
                    if date_format == "year_month":
                        date_str = time.strftime("%Y-%m", time.localtime(mtime))
                    elif date_format == "year":
                        date_str = time.strftime("%Y", time.localtime(mtime))
                    elif date_format == "year_month_day":
                        date_str = time.strftime("%Y-%m-%d", time.localtime(mtime))
                    else:
                        date_str = "unknown"
                    
                    date_dir = target_dir / date_str
                    date_dir.mkdir(exist_ok=True)
                    
                    target_file_path = date_dir / file_path.name
                    self._handle_duplicate_file(file_path, target_file_path)
                    file_counts[date_str] += 1
                    
                except Exception as e:
                    logger.error(f"Error organizing {file_path} by date: {e}")
        
        logger.info(f"Organized {sum(file_counts.values())} files by date")
        return dict(file_counts)
    
    def organize_by_size(self, source_dir: Path, target_dir: Optional[Path] = None) -> Dict[str, int]:
        """Organize files by their size ranges."""
        if not target_dir:
            target_dir = source_dir / "organized_by_size"
        
        target_dir.mkdir(exist_ok=True)
        
        size_categories = {
            'tiny': (0, CONSTANT_1024),  # < 1KB
            'small': (CONSTANT_1024, CONSTANT_1024*CONSTANT_1024),  # 1KB - 1MB
            'medium': (CONSTANT_1024*CONSTANT_1024, 10*CONSTANT_1024*CONSTANT_1024),  # 1MB - 10MB
            'large': (10*CONSTANT_1024*CONSTANT_1024, CONSTANT_100*CONSTANT_1024*CONSTANT_1024),  # 10MB - 100MB
            'huge': (CONSTANT_100*CONSTANT_1024*CONSTANT_1024, float('inf'))  # > 100MB
        }
        
        # Create size category directories
        for category in size_categories.keys():
            (target_dir / category).mkdir(exist_ok=True)
        
        file_counts = defaultdict(int)
        
        for file_path in source_dir.rglob("*"):
            if file_path.is_file():
                try:
                    size = file_path.stat().st_size
                    category = self._get_size_category(size, size_categories)
                    
                    category_dir = target_dir / category
                    target_file_path = category_dir / file_path.name
                    self._handle_duplicate_file(file_path, target_file_path)
                    file_counts[category] += 1
                    
                except Exception as e:
                    logger.error(f"Error organizing {file_path} by size: {e}")
        
        logger.info(f"Organized {sum(file_counts.values())} files by size")
        return dict(file_counts)
    
    # ============================================================================
    # DEDUPLICATION METHODS (from Robust Sort and Dedupe)
    # ============================================================================
    
    def calculate_file_hash(self, file_path: Path) -> str:
        """Calculate MD5 hash of file content."""
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
            return hashlib.md5(content).hexdigest()
        except Exception as e:
            logger.warning(f"Could not calculate hash for {file_path}: {e}")
            return ""
    
    def find_exact_duplicates(self, source_dir: Path) -> Dict[str, List[Path]]:
        """Find files with identical content."""
        logger.info("Scanning for exact duplicates...")
        
        file_hashes = defaultdict(list)
        
        for file_path in source_dir.rglob("*"):
            if file_path.is_file():
                file_hash = self.calculate_file_hash(file_path)
                if file_hash:
                    file_hashes[file_hash].append(file_path)
        
        # Return only groups with duplicates
        duplicates = {h: files for h, files in file_hashes.items() if len(files) > 1}
        logger.info(f"Found {len(duplicates)} groups of exact duplicates")
        return duplicates
    
    def find_similar_files(self, source_dir: Path) -> Dict[str, List[Path]]:
        """Find files with similar names and purposes."""
        logger.info("Scanning for similar files...")
        
        similar_groups = defaultdict(list)
        
        for file_path in source_dir.rglob("*.py"):
            if file_path.is_file():
                filename = file_path.name.lower()
                
                for pattern_name, patterns in self.similarity_patterns.items():
                    for pattern in patterns:
                        if re.search(pattern, filename):
                            similar_groups[pattern_name].append(file_path)
                            break
        
        # Filter out single-file groups
        similar_groups = {k: v for k, v in similar_groups.items() if len(v) > 1}
        logger.info(f"Found {len(similar_groups)} groups of similar files")
        return similar_groups
    
    def analyze_file_content(self, file_path: Path) -> Dict:
        """Analyze file content to determine category and quality."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Basic content analysis
            lines = content.split('\n')
            non_empty_lines = [line for line in lines if line.strip()]
            
            # Quality indicators
            has_docstring = '"""' in content or "'''" in content
            has_logging = 'logging' in content.lower()
            has_error_handling = 'try:' in content and 'except' in content
            has_main_function = 'if __name__ == "__main__":' in content
            has_imports = len([line for line in lines if line.strip().startswith('import') or line.strip().startswith('from')])
            
            # Determine category based on content
            category = self._determine_category(content, file_path.name)
            
            # Calculate quality score
            quality_score = 0
            if has_docstring: quality_score += 2
            if has_logging: quality_score += 1
            if has_error_handling: quality_score += 2
            if has_main_function: quality_score += 1
            if has_imports > 5: quality_score += 1
            
            return {
                'file_path': file_path,
                'category': category,
                'quality_score': quality_score,
                'lines': len(lines),
                'non_empty_lines': len(non_empty_lines),
                'has_docstring': has_docstring,
                'has_logging': has_logging,
                'has_error_handling': has_error_handling,
                'has_main_function': has_main_function,
                'imports_count': has_imports,
                'size': file_path.stat().st_size,
                'modified': file_path.stat().st_mtime
            }
            
        except Exception as e:
            logger.warning(f"Could not analyze {file_path}: {e}")
            return {
                'file_path': file_path,
                'category': 'utilities',
                'quality_score': 0,
                'lines': 0,
                'non_empty_lines': 0,
                'has_docstring': False,
                'has_logging': False,
                'has_error_handling': False,
                'has_main_function': False,
                'imports_count': 0,
                'size': 0,
                'modified': 0
            }
    
    def clean_duplicates(self, duplicates: Dict[str, List[Path]], 
                        keep_strategy: str = "largest", archive_dir: Optional[Path] = None) -> int:
        """Clean duplicate files using specified strategy."""
        if not archive_dir:
            archive_dir = Path("DUPLICATES_ARCHIVE")
        archive_dir.mkdir(exist_ok=True)
        
        removed_count = 0
        
        for file_hash, files in duplicates.items():
            if len(files) > 1:
                if keep_strategy == "largest":
                    files.sort(key=lambda x: x.stat().st_size, reverse=True)
                elif keep_strategy == "newest":
                    files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                elif keep_strategy == "shortest_path":
                    files.sort(key=lambda x: len(x.parts))
                elif keep_strategy == "highest_quality":
                    file_analyses = [self.analyze_file_content(f) for f in files]
                    file_analyses.sort(key=lambda x: x['quality_score'], reverse=True)
                    files = [f['file_path'] for f in file_analyses]
                
                keep_file = files[0]
                duplicates_to_remove = files[1:]
                
                logger.info(f"Keeping: {keep_file}")
                logger.info(f"Archiving {len(duplicates_to_remove)} duplicates")
                
                for duplicate in duplicates_to_remove:
                    try:
                        archive_path = archive_dir / duplicate.name
                        # Handle duplicate names in archive
                        counter = 1
                        original_archive = archive_path
                        while archive_path.exists():
                            stem = original_archive.stem
                            suffix = original_archive.suffix
                            archive_path = original_archive.parent / f"{stem}_{counter}{suffix}"
                            counter += 1
                        
                        shutil.move(str(duplicate), str(archive_path))
                        removed_count += 1
                        logger.info(f"Archived: {duplicate}")
                    except Exception as e:
                        logger.error(f"Error archiving {duplicate}: {e}")
        
        logger.info(f"Archived {removed_count} duplicate files")
        return removed_count
    
    # ============================================================================
    # COMPREHENSIVE WORKFLOW METHODS
    # ============================================================================
    
    def comprehensive_cleanup(self, source_dir: Path, target_dir: Optional[Path] = None) -> Dict:
        """Perform comprehensive cleanup: deduplicate, organize, and process files."""
        if not target_dir:
            target_dir = source_dir / "CLEANED_ORGANIZED"
        
        target_dir.mkdir(exist_ok=True)
        
        logger.info("Starting comprehensive cleanup...")
        
        # Step 1: Find and clean duplicates
        duplicates = self.find_exact_duplicates(source_dir)
        similar_files = self.find_similar_files(source_dir)
        
        # Clean duplicates
        archived_duplicates = self.clean_duplicates(duplicates, "highest_quality", target_dir / "DUPLICATES_ARCHIVE")
        
        # Step 2: Organize by file type
        type_organization = self.organize_by_file_type(source_dir, target_dir / "by_type")
        
        # Step 3: Organize music files specifically
        music_files = list(source_dir.rglob("*.mp3"))
        if music_files:
            music_organization = self.organize_music_library(source_dir, "artist", target_dir / "music_by_artist")
        else:
            music_organization = {}
        
        # Step 4: Generate comprehensive report
        report = {
            'timestamp': datetime.now().isoformat(),
            'source_directory': str(source_dir),
            'target_directory': str(target_dir),
            'duplicates_found': len(duplicates),
            'duplicates_archived': archived_duplicates,
            'similar_groups': len(similar_files),
            'type_organization': type_organization,
            'music_organization': music_organization,
            'total_files_processed': sum(type_organization.values()) + sum(music_organization.values())
        }
        
        # Save report
        report_path = target_dir / "CLEANUP_REPORT.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info("Comprehensive cleanup completed!")
        return report
    
    # ============================================================================
    # UTILITY METHODS
    # ============================================================================
    
    def _get_file_category(self, extension: str) -> str:
        """Get category for file extension."""
        for category, extensions in self.file_type_categories.items():
            if extension in extensions:
                return category
        return "other"
    
    def _get_size_category(self, size: int, size_categories: Dict) -> str:
        """Get size category for file size."""
        for category, (min_size, max_size) in size_categories.items():
            if min_size <= size < max_size:
                return category
        return "huge"
    
    def _extract_artist_from_filename(self, filename: str) -> str:
        """Extract artist name from filename."""
        # Try different patterns
        for pattern_name, pattern in self.music_organize_patterns.items():
            match = re.match(pattern, filename)
            if match:
                return match.group(1).strip()
        
        # Fallback: use first part before any separator
        separators = [' - ', ' _ ', ' -', '_']
        for sep in separators:
            if sep in filename:
                return filename.split(sep)[0].strip()
        
        return "Unknown Artist"
    
    def _extract_artist_album_from_filename(self, filename: str) -> Tuple[str, str]:
        """Extract artist and album from filename."""
        # Try artist - album - title pattern
        match = re.match(self.music_organize_patterns['artist_album'], filename)
        if match:
            return match.group(1).strip(), match.group(2).strip()
        
        # Fallback
        return self._extract_artist_from_filename(filename), "Unknown Album"
    
    def _sanitize_filename(self, filename: str) -> str:
        """Sanitize filename for filesystem use."""
        # Remove invalid characters
        invalid_chars = '<>:"/\\|?*'
        for char in invalid_chars:
            filename = filename.replace(char, '_')
        
        # Remove extra spaces and dots
        filename = re.sub(r'\s+', ' ', filename).strip()
        filename = filename.strip('.')
        
        return filename
    
    def _handle_duplicate_file(self, source: Path, target: Path):
        """Handle duplicate file names when moving."""
        counter = 1
        original_target = target
        while target.exists():
            stem = original_target.stem
            suffix = original_target.suffix
            target = original_target.parent / f"{stem}_{counter}{suffix}"
            counter += 1
        
        shutil.move(str(source), str(target))
    
    def _determine_category(self, content: str, filename: str) -> str:
        """Determine file category based on content and filename."""
        content_lower = content.lower()
        filename_lower = filename.lower()
        
        # Analysis scripts
        if any(keyword in content_lower for keyword in ['analyze', 'analysis', 'analyzer']):
            return 'core_analysis'
        
        # Transcription scripts
        if any(keyword in content_lower for keyword in ['transcribe', 'transcript', 'whisper', 'speech']):
            return 'transcription'
        
        # Generation scripts
        if any(keyword in content_lower for keyword in ['generate', 'create', 'build', 'html', 'csv']):
            return 'generation'
        
        # Processing scripts
        if any(keyword in content_lower for keyword in ['process', 'convert', 'mp3', 'mp4', 'ffmpeg']):
            return 'processing'
        
        # Web scraping scripts
        if any(keyword in content_lower for keyword in ['scrape', 'suno', 'beautifulsoup', 'requests']):
            return 'web_scraping'
        
        # Organization scripts
        if any(keyword in content_lower for keyword in ['organize', 'sort', 'manage', 'file']):
            return 'organization'
        
        # Experimental/test scripts
        if any(keyword in filename_lower for keyword in ['test', 'experimental', 'untitled', 'copy']):
            return 'experimental'
        
        return 'utilities'

    """get_openai_api_key function."""

# From analyze_analyze__merged.py
def get_openai_api_key():
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        api_key = input("Enter your OpenAI API key: ").strip()
    return api_key
    """get_directory_path function."""


# From analyze_analyze__merged.py
def get_directory_path(prompt_message, default_path):
    directory = input(f"{prompt_message} (default: {default_path}): ").strip()
    """get_pydocgen_paths function."""

    return directory if directory else default_path

# From analyze_analyze__merged.py
def get_pydocgen_paths():
    python_directory = get_directory_path(
        "Enter the directory where Python scripts are located",
        Path("/Users/steven/Documents/python"),
    )
    docs_directory = get_directory_path(
        "Enter the directory where documentation should be saved",
        Path("/Users/steven/Documents/python/docs"),
    """generate_docs function."""

    )
    return python_directory, docs_directory

# From analyze_analyze__merged.py
def generate_docs():
    python_directory, docs_directory = get_pydocgen_paths()

    # Ensure the docs directory exists; if not, create it
    if not os.path.exists(docs_directory):
        os.makedirs(docs_directory)
        logger.info(f"Created missing directory: {docs_directory}")

    try:
        subprocess.run(
            ["pydocgen", "-i", python_directory, "-o", docs_directory], check=True
        )
    """enhance_docs function."""

        logger.info(f"Documentation generated in {docs_directory}")
    except subprocess.CalledProcessError as e:
        logger.info(f"Error running pydocgen: {e}")

# From analyze_analyze__merged.py
def enhance_docs():
    python_directory, docs_directory = get_pydocgen_paths()

    for filename in os.listdir(docs_directory):
        if filename.endswith(".md") or filename.endswith(".rst"):
            file_path = os.path.join(docs_directory, filename)
            enhanced_file_path = os.path.join(docs_directory, f"enhanced_{filename}")

            # Skip enhancement if the enhanced file already exists
            if os.path.exists(enhanced_file_path):
                logger.info(f"Enhanced file already exists: {enhanced_file_path}")
                continue

            with open(file_path, "r") as doc_file:
                content = doc_file.read()

                # Use OpenAI to enhance the documentation
                response = openai.Completion.create(
                    engine="text-davinci-003",
                    prompt=f"Enhance the following Python code documentation for better readability and detail:\n\n{content}",
                    max_tokens=CONSTANT_1000,
                )

                enhanced_content = response.choices[0].text.strip()

            # Write the enhanced documentation to a new file
    """run_flake8 function."""

            with open(enhanced_file_path, "w") as enhanced_doc_file:
                enhanced_doc_file.write(enhanced_content)

            logger.info(f"Enhanced documentation saved to: {enhanced_file_path}")

# From analyze_analyze__merged.py
def run_flake8():
    python_directory, docs_directory = get_pydocgen_paths()

    flake8_report_file = os.path.join(docs_directory, "flake8_report.txt")

    # Skip flake8 if the report already exists
    if os.path.exists(flake8_report_file):
        logger.info(f"flake8 report already exists: {flake8_report_file}")
        return

    try:
        result = subprocess.run(
            ["flake8", python_directory, "--max-line-length=88"],
            check=False,
            capture_output=True,
            text=True,
        )
        if result.returncode == 0:
            logger.info("All files passed flake8 checks.")
        else:
            logger.info("flake8 found issues:")
    """get_cover_images function."""

            with open(flake8_report_file, "w") as report_file:
                report_file.write(result.stdout)
            logger.info(f"flake8 issues saved to: {flake8_report_file}")
    except subprocess.CalledProcessError as e:
        logger.info(f"Error running flake8: {e}")

# From merged_7_files.py
def get_cover_images(file_name, cover_image_directory):
    # Check for both JPG and PNG extensions
    images = []
    jpg_paths = glob.glob(os.path.join(cover_image_directory, f"{file_name}*.jpg"))
    png_paths = glob.glob(os.path.join(cover_image_directory, f"{file_name}*.png"))

    images.extend(jpg_paths)
    images.extend(png_paths)

    if images:
    """convert_mp3_to_mp4_with_images function."""

        return images
    else:
        print(
            f"Cover images not found for {file_name}. Please ensure the cover images exist."
        )
        return None

# From merged_7_files.py
def convert_mp3_to_mp4_with_images(mp3_file, cover_images, output_file):
    audio = AudioFileClip(mp3_file)
    clips = [
    """save_transcription function."""

        ImageClip(image).set_duration(audio.duration / len(cover_images))
        for image in cover_images
    ]
    video = ImageSequenceClip(clips, fps=1)  # 1 fps as each image is a frame
    video = video.set_duration(audio.duration)
    video = video.set_audio(audio)
    video.write_videofile(output_file, fps=24)

# From merged_3_files.py
def save_transcription(segments, output_file):
    with open(output_file, "w") as f:
        for segment in segments:
            start = segment["start"]
            end = segment["end"]
            text = segment["text"]
            f.write(f"[{start:.2f} - {end:.2f}] {text}\n")
    logger.info(f"Transcription saved to {output_file}")